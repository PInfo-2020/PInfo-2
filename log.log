docker-compose_pgdata-usr
Creating network "docker-compose_backend-network" with driver "bridge"
Creating volume "docker-compose_pgdata-usr" with default driver
Creating user-database ... 
Creating zookeeper     ... 
[1A[2KCreating zookeeper     ... [32mdone[0m[1BCreating kafka         ... 
[3A[2KCreating user-database ... [32mdone[0m[3B[1A[2KCreating kafka         ... [32mdone[0m[1BCreating user-service  ... 
[1A[2KCreating user-service  ... [32mdone[0m[1BAttaching to zookeeper, user-database, kafka, user-service
[36mkafka            |[0m ===> ENV Variables ...
[36mkafka            |[0m ALLOW_UNSIGNED=false
[36mkafka            |[0m COMPONENT=kafka
[36mkafka            |[0m CONFLUENT_DEB_VERSION=1
[36mkafka            |[0m CONFLUENT_MAJOR_VERSION=5
[36mkafka            |[0m CONFLUENT_MINOR_VERSION=1
[36mkafka            |[0m CONFLUENT_MVN_LABEL=
[36mkafka            |[0m CONFLUENT_PATCH_VERSION=0
[36mkafka            |[0m CONFLUENT_PLATFORM_LABEL=
[36mkafka            |[0m CONFLUENT_VERSION=5.1.0
[36mkafka            |[0m CUB_CLASSPATH=/etc/confluent/docker/docker-utils.jar
[36mkafka            |[0m HOME=/root
[36mkafka            |[0m HOSTNAME=kafka
[36mkafka            |[0m KAFKA_ADVERTISED_LISTENERS=LISTENER_DOCKER_INTERNAL://kafka:19092,LISTENER_DOCKER_EXTERNAL://kafka:9092
[36mkafka            |[0m KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
[36mkafka            |[0m KAFKA_BROKER_ID=1
[36mkafka            |[0m KAFKA_INTER_BROKER_LISTENER_NAME=LISTENER_DOCKER_INTERNAL
[36mkafka            |[0m KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
[36mkafka            |[0m KAFKA_LOG4J_LOGGERS=kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO
[36mkafka            |[0m KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
[36mkafka            |[0m KAFKA_VERSION=2.1.0
[36mkafka            |[0m KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
[36mkafka            |[0m LANG=C.UTF-8
[36mkafka            |[0m PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
[36mkafka            |[0m PWD=/
[36mkafka            |[0m PYTHON_PIP_VERSION=8.1.2
[36mkafka            |[0m PYTHON_VERSION=2.7.9-1
[36mkafka            |[0m SCALA_VERSION=2.11
[36mkafka            |[0m SHLVL=1
[36mkafka            |[0m ZULU_OPENJDK_VERSION=8=8.30.0.1
[36mkafka            |[0m _=/usr/bin/env
[36mkafka            |[0m ===> User
[36mkafka            |[0m uid=0(root) gid=0(root) groups=0(root)
[36mkafka            |[0m ===> Configuring ...
[33muser-database    |[0m The files belonging to this database system will be owned by user "postgres".
[33muser-database    |[0m This user must also own the server process.
[33muser-database    |[0m 
[33muser-database    |[0m The database cluster will be initialized with locale "en_US.utf8".
[33muser-database    |[0m The default database encoding has accordingly been set to "UTF8".
[33muser-database    |[0m The default text search configuration will be set to "english".
[33muser-database    |[0m 
[33muser-database    |[0m Data page checksums are disabled.
[33muser-database    |[0m 
[33muser-database    |[0m fixing permissions on existing directory /var/lib/postgresql/data ... ok
[33muser-database    |[0m creating subdirectories ... ok
[33muser-database    |[0m selecting default max_connections ... 100
[33muser-database    |[0m selecting default shared_buffers ... 128MB
[33muser-database    |[0m selecting default timezone ... Etc/UTC
[33muser-database    |[0m selecting dynamic shared memory implementation ... posix
[33muser-database    |[0m creating configuration files ... ok
[33muser-database    |[0m running bootstrap script ... ok
[33muser-database    |[0m performing post-bootstrap initialization ... ok
[35mzookeeper        |[0m ZooKeeper JMX enabled by default
[35mzookeeper        |[0m Using config: /conf/zoo.cfg
[35mzookeeper        |[0m 2020-05-28 13:01:18,470 [myid:] - INFO  [main:QuorumPeerConfig@124] - Reading configuration from: /conf/zoo.cfg
[35mzookeeper        |[0m 2020-05-28 13:01:18,580 [myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: zookeeper to address: zookeeper/172.22.0.3
[35mzookeeper        |[0m 2020-05-28 13:01:18,586 [myid:] - ERROR [main:QuorumPeerConfig@301] - Invalid configuration, only one server specified (ignoring)
[35mzookeeper        |[0m 2020-05-28 13:01:18,598 [myid:] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
[35mzookeeper        |[0m 2020-05-28 13:01:18,600 [myid:] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
[35mzookeeper        |[0m 2020-05-28 13:01:18,600 [myid:] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.
[35mzookeeper        |[0m 2020-05-28 13:01:18,600 [myid:] - WARN  [main:QuorumPeerMain@113] - Either no config or no quorum defined in config, running  in standalone mode
[35mzookeeper        |[0m 2020-05-28 13:01:18,674 [myid:] - INFO  [main:QuorumPeerConfig@124] - Reading configuration from: /conf/zoo.cfg
[35mzookeeper        |[0m 2020-05-28 13:01:18,677 [myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: zookeeper to address: zookeeper/172.22.0.3
[35mzookeeper        |[0m 2020-05-28 13:01:18,678 [myid:] - ERROR [main:QuorumPeerConfig@301] - Invalid configuration, only one server specified (ignoring)
[35mzookeeper        |[0m 2020-05-28 13:01:18,679 [myid:] - INFO  [main:ZooKeeperServerMain@96] - Starting server
[35mzookeeper        |[0m 2020-05-28 13:01:18,746 [myid:] - INFO  [main:Environment@100] - Server environment:zookeeper.version=3.4.9-1757313, built on 08/23/2016 06:50 GMT
[35mzookeeper        |[0m 2020-05-28 13:01:18,747 [myid:] - INFO  [main:Environment@100] - Server environment:host.name=zookeeper
[35mzookeeper        |[0m 2020-05-28 13:01:18,748 [myid:] - INFO  [main:Environment@100] - Server environment:java.version=1.8.0_121
[35mzookeeper        |[0m 2020-05-28 13:01:18,748 [myid:] - INFO  [main:Environment@100] - Server environment:java.vendor=Oracle Corporation
[35mzookeeper        |[0m 2020-05-28 13:01:18,749 [myid:] - INFO  [main:Environment@100] - Server environment:java.home=/usr/lib/jvm/java-1.8-openjdk/jre
[35mzookeeper        |[0m 2020-05-28 13:01:18,752 [myid:] - INFO  [main:Environment@100] - Server environment:java.class.path=/zookeeper-3.4.9/bin/../build/classes:/zookeeper-3.4.9/bin/../build/lib/*.jar:/zookeeper-3.4.9/bin/../lib/slf4j-log4j12-1.6.1.jar:/zookeeper-3.4.9/bin/../lib/slf4j-api-1.6.1.jar:/zookeeper-3.4.9/bin/../lib/netty-3.10.5.Final.jar:/zookeeper-3.4.9/bin/../lib/log4j-1.2.16.jar:/zookeeper-3.4.9/bin/../lib/jline-0.9.94.jar:/zookeeper-3.4.9/bin/../zookeeper-3.4.9.jar:/zookeeper-3.4.9/bin/../src/java/lib/*.jar:/conf:
[35mzookeeper        |[0m 2020-05-28 13:01:18,763 [myid:] - INFO  [main:Environment@100] - Server environment:java.library.path=/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[35mzookeeper        |[0m 2020-05-28 13:01:18,763 [myid:] - INFO  [main:Environment@100] - Server environment:java.io.tmpdir=/tmp
[35mzookeeper        |[0m 2020-05-28 13:01:18,764 [myid:] - INFO  [main:Environment@100] - Server environment:java.compiler=<NA>
[35mzookeeper        |[0m 2020-05-28 13:01:18,764 [myid:] - INFO  [main:Environment@100] - Server environment:os.name=Linux
[35mzookeeper        |[0m 2020-05-28 13:01:18,784 [myid:] - INFO  [main:Environment@100] - Server environment:os.arch=amd64
[35mzookeeper        |[0m 2020-05-28 13:01:18,785 [myid:] - INFO  [main:Environment@100] - Server environment:os.version=5.4.0-31-generic
[35mzookeeper        |[0m 2020-05-28 13:01:18,785 [myid:] - INFO  [main:Environment@100] - Server environment:user.name=zookeeper
[35mzookeeper        |[0m 2020-05-28 13:01:18,786 [myid:] - INFO  [main:Environment@100] - Server environment:user.home=/home/zookeeper
[35mzookeeper        |[0m 2020-05-28 13:01:18,786 [myid:] - INFO  [main:Environment@100] - Server environment:user.dir=/zookeeper-3.4.9
[35mzookeeper        |[0m 2020-05-28 13:01:18,892 [myid:] - INFO  [main:ZooKeeperServer@815] - tickTime set to 2000
[35mzookeeper        |[0m 2020-05-28 13:01:18,893 [myid:] - INFO  [main:ZooKeeperServer@824] - minSessionTimeout set to -1
[35mzookeeper        |[0m 2020-05-28 13:01:18,893 [myid:] - INFO  [main:ZooKeeperServer@833] - maxSessionTimeout set to -1
[35mzookeeper        |[0m 2020-05-28 13:01:19,010 [myid:] - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2181
[33muser-database    |[0m syncing data to disk ... ok
[33muser-database    |[0m 
[33muser-database    |[0m WARNING: enabling "trust" authentication for local connections
[33muser-database    |[0m You can change this by editing pg_hba.conf or using the option -A, or
[33muser-database    |[0m --auth-local and --auth-host, the next time you run initdb.
[33muser-database    |[0m 
[33muser-database    |[0m Success. You can now start the database server using:
[33muser-database    |[0m 
[33muser-database    |[0m     pg_ctl -D /var/lib/postgresql/data -l logfile start
[33muser-database    |[0m 
[33muser-database    |[0m waiting for server to start....2020-05-28 13:01:20.756 UTC [45] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
[33muser-database    |[0m 2020-05-28 13:01:20.818 UTC [46] LOG:  database system was shut down at 2020-05-28 13:01:19 UTC
[33muser-database    |[0m 2020-05-28 13:01:20.834 UTC [45] LOG:  database system is ready to accept connections
[33muser-database    |[0m  done
[33muser-database    |[0m server started
[33muser-database    |[0m CREATE DATABASE
[33muser-database    |[0m 
[33muser-database    |[0m 
[33muser-database    |[0m /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/init.sql
[33muser-database    |[0m psql:/docker-entrypoint-initdb.d/init.sql:2: NOTICE:  table "auser" does not exist, skipping
[33muser-database    |[0m DROP TABLE
[33muser-database    |[0m psql:/docker-entrypoint-initdb.d/init.sql:3: NOTICE:  sequence "user_seq" does not exist, skipping
[33muser-database    |[0m DROP SEQUENCE
[33muser-database    |[0m CREATE SEQUENCE
[33muser-database    |[0m CREATE TABLE
[33muser-database    |[0m GRANT
[33muser-database    |[0m GRANT
[33muser-database    |[0m TRUNCATE TABLE
[33muser-database    |[0m INSERT 0 1
[33muser-database    |[0m INSERT 0 1
[33muser-database    |[0m 
[33muser-database    |[0m 
[33muser-database    |[0m 2020-05-28 13:01:22.180 UTC [45] LOG:  received fast shutdown request
[33muser-database    |[0m waiting for server to shut down....2020-05-28 13:01:22.192 UTC [45] LOG:  aborting any active transactions
[33muser-database    |[0m 2020-05-28 13:01:22.198 UTC [45] LOG:  worker process: logical replication launcher (PID 52) exited with exit code 1
[33muser-database    |[0m 2020-05-28 13:01:22.201 UTC [47] LOG:  shutting down
[33muser-database    |[0m 2020-05-28 13:01:22.360 UTC [45] LOG:  database system is shut down
[33muser-database    |[0m  done
[33muser-database    |[0m server stopped
[33muser-database    |[0m 
[33muser-database    |[0m PostgreSQL init process complete; ready for start up.
[33muser-database    |[0m 
[33muser-database    |[0m 2020-05-28 13:01:22.436 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
[33muser-database    |[0m 2020-05-28 13:01:22.436 UTC [1] LOG:  listening on IPv6 address "::", port 5432
[33muser-database    |[0m 2020-05-28 13:01:22.476 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
[33muser-database    |[0m 2020-05-28 13:01:22.566 UTC [72] LOG:  database system was shut down at 2020-05-28 13:01:22 UTC
[33muser-database    |[0m 2020-05-28 13:01:22.587 UTC [1] LOG:  database system is ready to accept connections
[36mkafka            |[0m ===> Running preflight checks ... 
[36mkafka            |[0m ===> Check if /var/lib/kafka/data is writable ...
[36mkafka            |[0m ===> Check if Zookeeper is healthy ...
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=kafka
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=1.8.0_172
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Azul Systems, Inc.
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/etc/confluent/docker/docker-utils.jar
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.version=5.4.0-31-generic
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.name=root
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.home=/root
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@b1bc7ed
[36mkafka            |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.22.0.3:2181. Will not attempt to authenticate using SASL (unknown error)
[36mkafka            |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established to zookeeper/172.22.0.3:2181, initiating session
[35mzookeeper        |[0m 2020-05-28 13:01:35,659 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /172.22.0.4:49802
[35mzookeeper        |[0m 2020-05-28 13:01:35,692 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@928] - Client attempting to establish new session at /172.22.0.4:49802
[35mzookeeper        |[0m 2020-05-28 13:01:35,696 [myid:] - INFO  [SyncThread:0:FileTxnLog@203] - Creating new log file: log.3ff
[35mzookeeper        |[0m 2020-05-28 13:01:35,713 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@673] - Established session 0x1725b5f6bbe0000 with negotiated timeout 40000 for client /172.22.0.4:49802
[36mkafka            |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server zookeeper/172.22.0.3:2181, sessionid = 0x1725b5f6bbe0000, negotiated timeout = 40000
[35mzookeeper        |[0m 2020-05-28 13:01:35,737 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@487] - Processed session termination for sessionid: 0x1725b5f6bbe0000
[35mzookeeper        |[0m 2020-05-28 13:01:35,747 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1008] - Closed socket connection for client /172.22.0.4:49802 which had sessionid 0x1725b5f6bbe0000
[36mkafka            |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Session: 0x1725b5f6bbe0000 closed
[36mkafka            |[0m ===> Launching ... 
[36mkafka            |[0m ===> Launching kafka ... 
[36mkafka            |[0m [2020-05-28 13:01:38,104] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[32muser-service     |[0m [0m2020-05-28 13:01:38,164 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:                   JAX-RS - STABLE          io.thorntail:jaxrs:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,275 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:                  Logging - STABLE          io.thorntail:logging:2.6.0.Final
[32muser-service     |[0m [0m[33m2020-05-28 13:01:38,276 WARN  [org.wildfly.swarm] (main) THORN0013: Installed fraction:                  Swagger - UNSTABLE        io.thorntail:swagger:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,276 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:        CDI Configuration - STABLE          io.thorntail:cdi-config:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,277 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:                 Undertow - STABLE          io.thorntail:undertow:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,277 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:                 Remoting - STABLE          io.thorntail:remoting:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,283 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:                      JMX - STABLE          io.thorntail:jmx:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,284 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:               Infinispan - STABLE          io.thorntail:infinispan:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,284 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:             Transactions - STABLE          io.thorntail:transactions:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,285 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:                      CDI - STABLE          io.thorntail:cdi:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,285 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:                      JCA - STABLE          io.thorntail:jca:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,296 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:                      JPA - STABLE          io.thorntail:jpa:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,297 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:                  Elytron - STABLE          io.thorntail:elytron:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,298 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:              Datasources - STABLE          io.thorntail:datasources:2.6.0.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:38,308 INFO  [org.wildfly.swarm] (main) THORN0013: Installed fraction:          Bean Validation - STABLE          io.thorntail:bean-validation:2.6.0.Final
[36mkafka            |[0m [2020-05-28 13:01:39,079] INFO KafkaConfig values: 
[36mkafka            |[0m 	advertised.host.name = null
[36mkafka            |[0m 	advertised.listeners = LISTENER_DOCKER_INTERNAL://kafka:19092,LISTENER_DOCKER_EXTERNAL://kafka:9092
[36mkafka            |[0m 	advertised.port = null
[36mkafka            |[0m 	alter.config.policy.class.name = null
[36mkafka            |[0m 	alter.log.dirs.replication.quota.window.num = 11
[36mkafka            |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[36mkafka            |[0m 	authorizer.class.name = 
[36mkafka            |[0m 	auto.create.topics.enable = true
[36mkafka            |[0m 	auto.leader.rebalance.enable = true
[36mkafka            |[0m 	background.threads = 10
[36mkafka            |[0m 	broker.id = 1
[36mkafka            |[0m 	broker.id.generation.enable = true
[36mkafka            |[0m 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
[36mkafka            |[0m 	broker.rack = null
[36mkafka            |[0m 	client.quota.callback.class = null
[36mkafka            |[0m 	compression.type = producer
[36mkafka            |[0m 	connection.failed.authentication.delay.ms = 100
[36mkafka            |[0m 	connections.max.idle.ms = 600000
[36mkafka            |[0m 	controlled.shutdown.enable = true
[36mkafka            |[0m 	controlled.shutdown.max.retries = 3
[36mkafka            |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[36mkafka            |[0m 	controller.socket.timeout.ms = 30000
[36mkafka            |[0m 	create.topic.policy.class.name = null
[36mkafka            |[0m 	default.replication.factor = 1
[36mkafka            |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[36mkafka            |[0m 	delegation.token.expiry.time.ms = 86400000
[36mkafka            |[0m 	delegation.token.master.key = null
[36mkafka            |[0m 	delegation.token.max.lifetime.ms = 604800000
[36mkafka            |[0m 	delete.records.purgatory.purge.interval.requests = 1
[36mkafka            |[0m 	delete.topic.enable = true
[36mkafka            |[0m 	fetch.purgatory.purge.interval.requests = 1000
[36mkafka            |[0m 	group.initial.rebalance.delay.ms = 3000
[36mkafka            |[0m 	group.max.session.timeout.ms = 300000
[36mkafka            |[0m 	group.min.session.timeout.ms = 6000
[36mkafka            |[0m 	host.name = 
[36mkafka            |[0m 	inter.broker.listener.name = LISTENER_DOCKER_INTERNAL
[36mkafka            |[0m 	inter.broker.protocol.version = 2.1-IV2
[36mkafka            |[0m 	kafka.metrics.polling.interval.secs = 10
[36mkafka            |[0m 	kafka.metrics.reporters = []
[36mkafka            |[0m 	leader.imbalance.check.interval.seconds = 300
[36mkafka            |[0m 	leader.imbalance.per.broker.percentage = 10
[36mkafka            |[0m 	listener.security.protocol.map = LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
[36mkafka            |[0m 	listeners = LISTENER_DOCKER_INTERNAL://0.0.0.0:19092,LISTENER_DOCKER_EXTERNAL://0.0.0.0:9092
[36mkafka            |[0m 	log.cleaner.backoff.ms = 15000
[36mkafka            |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[36mkafka            |[0m 	log.cleaner.delete.retention.ms = 86400000
[36mkafka            |[0m 	log.cleaner.enable = true
[36mkafka            |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[36mkafka            |[0m 	log.cleaner.io.buffer.size = 524288
[36mkafka            |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[36mkafka            |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[36mkafka            |[0m 	log.cleaner.min.compaction.lag.ms = 0
[36mkafka            |[0m 	log.cleaner.threads = 1
[36mkafka            |[0m 	log.cleanup.policy = [delete]
[36mkafka            |[0m 	log.dir = /tmp/kafka-logs
[36mkafka            |[0m 	log.dirs = /var/lib/kafka/data
[36mkafka            |[0m 	log.flush.interval.messages = 9223372036854775807
[36mkafka            |[0m 	log.flush.interval.ms = null
[36mkafka            |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[36mkafka            |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[36mkafka            |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[36mkafka            |[0m 	log.index.interval.bytes = 4096
[36mkafka            |[0m 	log.index.size.max.bytes = 10485760
[36mkafka            |[0m 	log.message.downconversion.enable = true
[36mkafka            |[0m 	log.message.format.version = 2.1-IV2
[36mkafka            |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[36mkafka            |[0m 	log.message.timestamp.type = CreateTime
[36mkafka            |[0m 	log.preallocate = false
[36mkafka            |[0m 	log.retention.bytes = -1
[36mkafka            |[0m 	log.retention.check.interval.ms = 300000
[36mkafka            |[0m 	log.retention.hours = 168
[36mkafka            |[0m 	log.retention.minutes = null
[36mkafka            |[0m 	log.retention.ms = null
[36mkafka            |[0m 	log.roll.hours = 168
[36mkafka            |[0m 	log.roll.jitter.hours = 0
[36mkafka            |[0m 	log.roll.jitter.ms = null
[36mkafka            |[0m 	log.roll.ms = null
[36mkafka            |[0m 	log.segment.bytes = 1073741824
[36mkafka            |[0m 	log.segment.delete.delay.ms = 60000
[36mkafka            |[0m 	max.connections.per.ip = 2147483647
[36mkafka            |[0m 	max.connections.per.ip.overrides = 
[36mkafka            |[0m 	max.incremental.fetch.session.cache.slots = 1000
[36mkafka            |[0m 	message.max.bytes = 1000012
[36mkafka            |[0m 	metric.reporters = []
[36mkafka            |[0m 	metrics.num.samples = 2
[36mkafka            |[0m 	metrics.recording.level = INFO
[36mkafka            |[0m 	metrics.sample.window.ms = 30000
[36mkafka            |[0m 	min.insync.replicas = 1
[36mkafka            |[0m 	num.io.threads = 8
[36mkafka            |[0m 	num.network.threads = 3
[36mkafka            |[0m 	num.partitions = 1
[36mkafka            |[0m 	num.recovery.threads.per.data.dir = 1
[36mkafka            |[0m 	num.replica.alter.log.dirs.threads = null
[36mkafka            |[0m 	num.replica.fetchers = 1
[36mkafka            |[0m 	offset.metadata.max.bytes = 4096
[36mkafka            |[0m 	offsets.commit.required.acks = -1
[36mkafka            |[0m 	offsets.commit.timeout.ms = 5000
[36mkafka            |[0m 	offsets.load.buffer.size = 5242880
[36mkafka            |[0m 	offsets.retention.check.interval.ms = 600000
[36mkafka            |[0m 	offsets.retention.minutes = 10080
[36mkafka            |[0m 	offsets.topic.compression.codec = 0
[36mkafka            |[0m 	offsets.topic.num.partitions = 50
[36mkafka            |[0m 	offsets.topic.replication.factor = 1
[36mkafka            |[0m 	offsets.topic.segment.bytes = 104857600
[36mkafka            |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[36mkafka            |[0m 	password.encoder.iterations = 4096
[36mkafka            |[0m 	password.encoder.key.length = 128
[36mkafka            |[0m 	password.encoder.keyfactory.algorithm = null
[36mkafka            |[0m 	password.encoder.old.secret = null
[36mkafka            |[0m 	password.encoder.secret = null
[36mkafka            |[0m 	port = 9092
[36mkafka            |[0m 	principal.builder.class = null
[36mkafka            |[0m 	producer.purgatory.purge.interval.requests = 1000
[36mkafka            |[0m 	queued.max.request.bytes = -1
[36mkafka            |[0m 	queued.max.requests = 500
[36mkafka            |[0m 	quota.consumer.default = 9223372036854775807
[36mkafka            |[0m 	quota.producer.default = 9223372036854775807
[36mkafka            |[0m 	quota.window.num = 11
[36mkafka            |[0m 	quota.window.size.seconds = 1
[36mkafka            |[0m 	replica.fetch.backoff.ms = 1000
[36mkafka            |[0m 	replica.fetch.max.bytes = 1048576
[36mkafka            |[0m 	replica.fetch.min.bytes = 1
[36mkafka            |[0m 	replica.fetch.response.max.bytes = 10485760
[36mkafka            |[0m 	replica.fetch.wait.max.ms = 500
[36mkafka            |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[36mkafka            |[0m 	replica.lag.time.max.ms = 10000
[36mkafka            |[0m 	replica.socket.receive.buffer.bytes = 65536
[36mkafka            |[0m 	replica.socket.timeout.ms = 30000
[36mkafka            |[0m 	replication.quota.window.num = 11
[36mkafka            |[0m 	replication.quota.window.size.seconds = 1
[36mkafka            |[0m 	request.timeout.ms = 30000
[36mkafka            |[0m 	reserved.broker.max.id = 1000
[36mkafka            |[0m 	sasl.client.callback.handler.class = null
[36mkafka            |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[36mkafka            |[0m 	sasl.jaas.config = null
[36mkafka            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka            |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[36mkafka            |[0m 	sasl.kerberos.service.name = null
[36mkafka            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka            |[0m 	sasl.login.callback.handler.class = null
[36mkafka            |[0m 	sasl.login.class = null
[36mkafka            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka            |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[36mkafka            |[0m 	sasl.server.callback.handler.class = null
[36mkafka            |[0m 	security.inter.broker.protocol = PLAINTEXT
[36mkafka            |[0m 	socket.receive.buffer.bytes = 102400
[36mkafka            |[0m 	socket.request.max.bytes = 104857600
[36mkafka            |[0m 	socket.send.buffer.bytes = 102400
[36mkafka            |[0m 	ssl.cipher.suites = []
[36mkafka            |[0m 	ssl.client.auth = none
[36mkafka            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[36mkafka            |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka            |[0m 	ssl.key.password = null
[36mkafka            |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka            |[0m 	ssl.keystore.location = null
[36mkafka            |[0m 	ssl.keystore.password = null
[36mkafka            |[0m 	ssl.keystore.type = JKS
[36mkafka            |[0m 	ssl.protocol = TLS
[36mkafka            |[0m 	ssl.provider = null
[36mkafka            |[0m 	ssl.secure.random.implementation = null
[36mkafka            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka            |[0m 	ssl.truststore.location = null
[36mkafka            |[0m 	ssl.truststore.password = null
[36mkafka            |[0m 	ssl.truststore.type = JKS
[36mkafka            |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[36mkafka            |[0m 	transaction.max.timeout.ms = 900000
[36mkafka            |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[36mkafka            |[0m 	transaction.state.log.load.buffer.size = 5242880
[36mkafka            |[0m 	transaction.state.log.min.isr = 2
[36mkafka            |[0m 	transaction.state.log.num.partitions = 50
[36mkafka            |[0m 	transaction.state.log.replication.factor = 3
[36mkafka            |[0m 	transaction.state.log.segment.bytes = 104857600
[36mkafka            |[0m 	transactional.id.expiration.ms = 604800000
[36mkafka            |[0m 	unclean.leader.election.enable = false
[36mkafka            |[0m 	zookeeper.connect = zookeeper:2181
[36mkafka            |[0m 	zookeeper.connection.timeout.ms = null
[36mkafka            |[0m 	zookeeper.max.in.flight.requests = 10
[36mkafka            |[0m 	zookeeper.session.timeout.ms = 6000
[36mkafka            |[0m 	zookeeper.set.acl = false
[36mkafka            |[0m 	zookeeper.sync.time.ms = 2000
[36mkafka            |[0m  (kafka.server.KafkaConfig)
[36mkafka            |[0m [2020-05-28 13:01:39,213] WARN The package io.confluent.support.metrics.collectors.FullCollector for collecting the full set of support metrics could not be loaded, so we are reverting to anonymous, basic metric collection. If you are a Confluent customer, please refer to the Confluent Platform documentation, section Proactive Support, on how to activate full metrics collection. (io.confluent.support.metrics.KafkaSupportConfig)
[36mkafka            |[0m [2020-05-28 13:01:39,261] WARN Please note that the support metrics collection feature ("Metrics") of Proactive Support is enabled.  With Metrics enabled, this broker is configured to collect and report certain broker and cluster metadata ("Metadata") about your use of the Confluent Platform (including without limitation, your remote internet protocol address) to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every 24hours.  This Metadata may be transferred to any country in which Confluent maintains facilities.  For a more in depth discussion of how Confluent processes such information, please read our Privacy Policy located at http://www.confluent.io/privacy. By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer, storage and use of Metadata by Confluent.  You can turn the Metrics feature off by setting `confluent.support.metrics.enable=false` in the broker configuration and restarting the broker.  See the Confluent Platform documentation for further information. (io.confluent.support.metrics.SupportedServerStartable)
[36mkafka            |[0m [2020-05-28 13:01:39,270] INFO starting (kafka.server.KafkaServer)
[36mkafka            |[0m [2020-05-28 13:01:39,272] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
[36mkafka            |[0m [2020-05-28 13:01:39,358] INFO [ZooKeeperClient] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[36mkafka            |[0m [2020-05-28 13:01:39,376] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,376] INFO Client environment:host.name=kafka (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,377] INFO Client environment:java.version=1.8.0_172 (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,377] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,377] INFO Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,377] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/scala-logging_2.11-3.9.0.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.1.0-cp1-test.jar:/usr/bin/../share/java/kafka/zookeeper-3.4.13.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.12.v20180830.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.9.7.jar:/usr/bin/../share/java/kafka/zstd-jni-1.3.5-4.jar:/usr/bin/../share/java/kafka/connect-json-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/maven-artifact-3.5.4.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.12.v20180830.jar:/usr/bin/../share/java/kafka/support-metrics-client-5.1.0.jar:/usr/bin/../share/java/kafka/scala-reflect-2.11.12.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b42.jar:/usr/bin/../share/java/kafka/httpmime-4.5.2.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/kafka-tools-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/javax.inject-2.5.0-b42.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/validation-api-1.1.0.Final.jar:/usr/bin/../share/java/kafka/avro-1.8.1.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/commons-validator-1.4.1.jar:/usr/bin/../share/java/kafka/commons-compress-1.8.1.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.9.7.jar:/usr/bin/../share/java/kafka/commons-beanutils-1.8.3.jar:/usr/bin/../share/java/kafka/scala-library-2.11.12.jar:/usr/bin/../share/java/kafka/jersey-server-2.27.jar:/usr/bin/../share/java/kafka/paranamer-2.7.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.12.v20180830.jar:/usr/bin/../share/java/kafka/commons-lang3-3.5.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/jersey-media-jaxb-2.27.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.11.jar:/usr/bin/../share/java/kafka/httpcore-4.4.4.jar:/usr/bin/../share/java/kafka/connect-runtime-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/jersey-common-2.27.jar:/usr/bin/../share/java/kafka/jackson-core-asl-1.9.13.jar:/usr/bin/../share/java/kafka/xz-1.5.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.27.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.1.0-cp1-scaladoc.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.12.v20180830.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.25.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.2.jar:/usr/bin/../share/java/kafka/lz4-java-1.5.0.jar:/usr/bin/../share/java/kafka/kafka-streams-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/support-metrics-common-5.1.0.jar:/usr/bin/../share/java/kafka/jackson-core-2.9.7.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.27.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.7.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.12.v20180830.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.12.v20180830.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.12.v20180830.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.25.jar:/usr/bin/../share/java/kafka/commons-collections-3.2.1.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.11-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/hk2-api-2.5.0-b42.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.7.jar:/usr/bin/../share/java/kafka/hk2-utils-2.5.0-b42.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.7.2.jar:/usr/bin/../share/java/kafka/javax.inject-1.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/zkclient-0.10.jar:/usr/bin/../share/java/kafka/commons-codec-1.9.jar:/usr/bin/../share/java/kafka/connect-file-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/jline-0.9.94.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.1.0-cp1-test-sources.jar:/usr/bin/../share/java/kafka/jackson-mapper-asl-1.9.13.jar:/usr/bin/../share/java/kafka/jackson-databind-2.9.7.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/connect-api-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.27.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/guava-20.0.jar:/usr/bin/../share/java/kafka/plexus-utils-3.1.0.jar:/usr/bin/../share/java/kafka/jersey-client-2.27.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.12.v20180830.jar:/usr/bin/../share/java/kafka/common-utils-5.1.0.jar:/usr/bin/../share/java/kafka/kafka-clients-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/netty-3.10.6.Final.jar:/usr/bin/../share/java/kafka/connect-transforms-2.1.0-cp1.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.12.v20180830.jar:/usr/bin/../share/java/kafka/commons-digester-1.8.1.jar:/usr/bin/../share/java/kafka/hk2-locator-2.5.0-b42.jar:/usr/bin/../share/java/kafka/commons-lang3-3.1.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.jar:/usr/bin/../share/java/kafka/javassist-3.22.0-CR2.jar:/usr/bin/../share/java/kafka/httpclient-4.5.2.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.1.0-cp1-sources.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.1.0-cp1-javadoc.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-5.14.2.jar:/usr/bin/../share/java/kafka/log4j-1.2.17.jar:/usr/bin/../share/java/confluent-support-metrics/*:/usr/share/java/confluent-support-metrics/* (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,382] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,382] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,382] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,382] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,382] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,382] INFO Client environment:os.version=5.4.0-31-generic (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,382] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,383] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,383] INFO Client environment:user.dir=/ (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,385] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@dd05255 (org.apache.zookeeper.ZooKeeper)
[36mkafka            |[0m [2020-05-28 13:01:39,416] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[36mkafka            |[0m [2020-05-28 13:01:39,421] INFO Opening socket connection to server zookeeper/172.22.0.3:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[36mkafka            |[0m [2020-05-28 13:01:39,435] INFO Socket connection established to zookeeper/172.22.0.3:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[35mzookeeper        |[0m 2020-05-28 13:01:39,433 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /172.22.0.4:49804
[35mzookeeper        |[0m 2020-05-28 13:01:39,442 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@928] - Client attempting to establish new session at /172.22.0.4:49804
[35mzookeeper        |[0m 2020-05-28 13:01:39,451 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@673] - Established session 0x1725b5f6bbe0001 with negotiated timeout 6000 for client /172.22.0.4:49804
[36mkafka            |[0m [2020-05-28 13:01:39,464] INFO Session establishment complete on server zookeeper/172.22.0.3:2181, sessionid = 0x1725b5f6bbe0001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[36mkafka            |[0m [2020-05-28 13:01:39,473] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[35mzookeeper        |[0m 2020-05-28 13:01:39,658 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0x1 zxid:0x402 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers
[35mzookeeper        |[0m 2020-05-28 13:01:39,705 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0x2 zxid:0x403 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
[35mzookeeper        |[0m 2020-05-28 13:01:39,713 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0x3 zxid:0x404 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics
[35mzookeeper        |[0m 2020-05-28 13:01:39,745 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0x4 zxid:0x405 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes
[35mzookeeper        |[0m 2020-05-28 13:01:39,751 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0x5 zxid:0x406 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics
[35mzookeeper        |[0m 2020-05-28 13:01:39,756 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0x6 zxid:0x407 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid
[35mzookeeper        |[0m 2020-05-28 13:01:39,765 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0x7 zxid:0x408 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification
[35mzookeeper        |[0m 2020-05-28 13:01:39,771 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0x8 zxid:0x409 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block
[35mzookeeper        |[0m 2020-05-28 13:01:39,780 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0x9 zxid:0x40a txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification
[35mzookeeper        |[0m 2020-05-28 13:01:39,788 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0xa zxid:0x40b txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
[35mzookeeper        |[0m 2020-05-28 13:01:39,795 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0xb zxid:0x40c txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients
[35mzookeeper        |[0m 2020-05-28 13:01:39,816 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0xc zxid:0x40d txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users
[35mzookeeper        |[0m 2020-05-28 13:01:39,826 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:create cxid:0xd zxid:0x40e txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers
[36mkafka            |[0m [2020-05-28 13:01:40,580] INFO Cluster ID = CaiFW9wnThKVEnKWH302Fw (kafka.server.KafkaServer)
[36mkafka            |[0m [2020-05-28 13:01:40,811] INFO KafkaConfig values: 
[36mkafka            |[0m 	advertised.host.name = null
[36mkafka            |[0m 	advertised.listeners = LISTENER_DOCKER_INTERNAL://kafka:19092,LISTENER_DOCKER_EXTERNAL://kafka:9092
[36mkafka            |[0m 	advertised.port = null
[36mkafka            |[0m 	alter.config.policy.class.name = null
[36mkafka            |[0m 	alter.log.dirs.replication.quota.window.num = 11
[36mkafka            |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[36mkafka            |[0m 	authorizer.class.name = 
[36mkafka            |[0m 	auto.create.topics.enable = true
[36mkafka            |[0m 	auto.leader.rebalance.enable = true
[36mkafka            |[0m 	background.threads = 10
[36mkafka            |[0m 	broker.id = 1
[36mkafka            |[0m 	broker.id.generation.enable = true
[36mkafka            |[0m 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
[36mkafka            |[0m 	broker.rack = null
[36mkafka            |[0m 	client.quota.callback.class = null
[36mkafka            |[0m 	compression.type = producer
[36mkafka            |[0m 	connection.failed.authentication.delay.ms = 100
[36mkafka            |[0m 	connections.max.idle.ms = 600000
[36mkafka            |[0m 	controlled.shutdown.enable = true
[36mkafka            |[0m 	controlled.shutdown.max.retries = 3
[36mkafka            |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[36mkafka            |[0m 	controller.socket.timeout.ms = 30000
[36mkafka            |[0m 	create.topic.policy.class.name = null
[36mkafka            |[0m 	default.replication.factor = 1
[36mkafka            |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[36mkafka            |[0m 	delegation.token.expiry.time.ms = 86400000
[36mkafka            |[0m 	delegation.token.master.key = null
[36mkafka            |[0m 	delegation.token.max.lifetime.ms = 604800000
[36mkafka            |[0m 	delete.records.purgatory.purge.interval.requests = 1
[36mkafka            |[0m 	delete.topic.enable = true
[36mkafka            |[0m 	fetch.purgatory.purge.interval.requests = 1000
[36mkafka            |[0m 	group.initial.rebalance.delay.ms = 3000
[36mkafka            |[0m 	group.max.session.timeout.ms = 300000
[36mkafka            |[0m 	group.min.session.timeout.ms = 6000
[36mkafka            |[0m 	host.name = 
[36mkafka            |[0m 	inter.broker.listener.name = LISTENER_DOCKER_INTERNAL
[36mkafka            |[0m 	inter.broker.protocol.version = 2.1-IV2
[36mkafka            |[0m 	kafka.metrics.polling.interval.secs = 10
[36mkafka            |[0m 	kafka.metrics.reporters = []
[36mkafka            |[0m 	leader.imbalance.check.interval.seconds = 300
[36mkafka            |[0m 	leader.imbalance.per.broker.percentage = 10
[36mkafka            |[0m 	listener.security.protocol.map = LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
[36mkafka            |[0m 	listeners = LISTENER_DOCKER_INTERNAL://0.0.0.0:19092,LISTENER_DOCKER_EXTERNAL://0.0.0.0:9092
[36mkafka            |[0m 	log.cleaner.backoff.ms = 15000
[36mkafka            |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[36mkafka            |[0m 	log.cleaner.delete.retention.ms = 86400000
[36mkafka            |[0m 	log.cleaner.enable = true
[36mkafka            |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[36mkafka            |[0m 	log.cleaner.io.buffer.size = 524288
[36mkafka            |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[36mkafka            |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[36mkafka            |[0m 	log.cleaner.min.compaction.lag.ms = 0
[36mkafka            |[0m 	log.cleaner.threads = 1
[36mkafka            |[0m 	log.cleanup.policy = [delete]
[36mkafka            |[0m 	log.dir = /tmp/kafka-logs
[36mkafka            |[0m 	log.dirs = /var/lib/kafka/data
[36mkafka            |[0m 	log.flush.interval.messages = 9223372036854775807
[36mkafka            |[0m 	log.flush.interval.ms = null
[36mkafka            |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[36mkafka            |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[36mkafka            |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[36mkafka            |[0m 	log.index.interval.bytes = 4096
[36mkafka            |[0m 	log.index.size.max.bytes = 10485760
[36mkafka            |[0m 	log.message.downconversion.enable = true
[36mkafka            |[0m 	log.message.format.version = 2.1-IV2
[36mkafka            |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[36mkafka            |[0m 	log.message.timestamp.type = CreateTime
[36mkafka            |[0m 	log.preallocate = false
[36mkafka            |[0m 	log.retention.bytes = -1
[36mkafka            |[0m 	log.retention.check.interval.ms = 300000
[36mkafka            |[0m 	log.retention.hours = 168
[36mkafka            |[0m 	log.retention.minutes = null
[36mkafka            |[0m 	log.retention.ms = null
[36mkafka            |[0m 	log.roll.hours = 168
[36mkafka            |[0m 	log.roll.jitter.hours = 0
[36mkafka            |[0m 	log.roll.jitter.ms = null
[36mkafka            |[0m 	log.roll.ms = null
[36mkafka            |[0m 	log.segment.bytes = 1073741824
[36mkafka            |[0m 	log.segment.delete.delay.ms = 60000
[36mkafka            |[0m 	max.connections.per.ip = 2147483647
[36mkafka            |[0m 	max.connections.per.ip.overrides = 
[36mkafka            |[0m 	max.incremental.fetch.session.cache.slots = 1000
[36mkafka            |[0m 	message.max.bytes = 1000012
[36mkafka            |[0m 	metric.reporters = []
[36mkafka            |[0m 	metrics.num.samples = 2
[36mkafka            |[0m 	metrics.recording.level = INFO
[36mkafka            |[0m 	metrics.sample.window.ms = 30000
[36mkafka            |[0m 	min.insync.replicas = 1
[36mkafka            |[0m 	num.io.threads = 8
[36mkafka            |[0m 	num.network.threads = 3
[36mkafka            |[0m 	num.partitions = 1
[36mkafka            |[0m 	num.recovery.threads.per.data.dir = 1
[36mkafka            |[0m 	num.replica.alter.log.dirs.threads = null
[36mkafka            |[0m 	num.replica.fetchers = 1
[36mkafka            |[0m 	offset.metadata.max.bytes = 4096
[36mkafka            |[0m 	offsets.commit.required.acks = -1
[36mkafka            |[0m 	offsets.commit.timeout.ms = 5000
[36mkafka            |[0m 	offsets.load.buffer.size = 5242880
[36mkafka            |[0m 	offsets.retention.check.interval.ms = 600000
[36mkafka            |[0m 	offsets.retention.minutes = 10080
[36mkafka            |[0m 	offsets.topic.compression.codec = 0
[36mkafka            |[0m 	offsets.topic.num.partitions = 50
[36mkafka            |[0m 	offsets.topic.replication.factor = 1
[36mkafka            |[0m 	offsets.topic.segment.bytes = 104857600
[36mkafka            |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[36mkafka            |[0m 	password.encoder.iterations = 4096
[36mkafka            |[0m 	password.encoder.key.length = 128
[36mkafka            |[0m 	password.encoder.keyfactory.algorithm = null
[36mkafka            |[0m 	password.encoder.old.secret = null
[36mkafka            |[0m 	password.encoder.secret = null
[36mkafka            |[0m 	port = 9092
[36mkafka            |[0m 	principal.builder.class = null
[36mkafka            |[0m 	producer.purgatory.purge.interval.requests = 1000
[36mkafka            |[0m 	queued.max.request.bytes = -1
[36mkafka            |[0m 	queued.max.requests = 500
[36mkafka            |[0m 	quota.consumer.default = 9223372036854775807
[36mkafka            |[0m 	quota.producer.default = 9223372036854775807
[36mkafka            |[0m 	quota.window.num = 11
[36mkafka            |[0m 	quota.window.size.seconds = 1
[36mkafka            |[0m 	replica.fetch.backoff.ms = 1000
[36mkafka            |[0m 	replica.fetch.max.bytes = 1048576
[36mkafka            |[0m 	replica.fetch.min.bytes = 1
[36mkafka            |[0m 	replica.fetch.response.max.bytes = 10485760
[36mkafka            |[0m 	replica.fetch.wait.max.ms = 500
[36mkafka            |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[36mkafka            |[0m 	replica.lag.time.max.ms = 10000
[36mkafka            |[0m 	replica.socket.receive.buffer.bytes = 65536
[36mkafka            |[0m 	replica.socket.timeout.ms = 30000
[36mkafka            |[0m 	replication.quota.window.num = 11
[36mkafka            |[0m 	replication.quota.window.size.seconds = 1
[36mkafka            |[0m 	request.timeout.ms = 30000
[36mkafka            |[0m 	reserved.broker.max.id = 1000
[36mkafka            |[0m 	sasl.client.callback.handler.class = null
[36mkafka            |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[36mkafka            |[0m 	sasl.jaas.config = null
[36mkafka            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka            |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[36mkafka            |[0m 	sasl.kerberos.service.name = null
[36mkafka            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka            |[0m 	sasl.login.callback.handler.class = null
[36mkafka            |[0m 	sasl.login.class = null
[36mkafka            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka            |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[36mkafka            |[0m 	sasl.server.callback.handler.class = null
[36mkafka            |[0m 	security.inter.broker.protocol = PLAINTEXT
[36mkafka            |[0m 	socket.receive.buffer.bytes = 102400
[36mkafka            |[0m 	socket.request.max.bytes = 104857600
[36mkafka            |[0m 	socket.send.buffer.bytes = 102400
[36mkafka            |[0m 	ssl.cipher.suites = []
[36mkafka            |[0m 	ssl.client.auth = none
[36mkafka            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[36mkafka            |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka            |[0m 	ssl.key.password = null
[36mkafka            |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka            |[0m 	ssl.keystore.location = null
[36mkafka            |[0m 	ssl.keystore.password = null
[36mkafka            |[0m 	ssl.keystore.type = JKS
[36mkafka            |[0m 	ssl.protocol = TLS
[36mkafka            |[0m 	ssl.provider = null
[36mkafka            |[0m 	ssl.secure.random.implementation = null
[36mkafka            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka            |[0m 	ssl.truststore.location = null
[36mkafka            |[0m 	ssl.truststore.password = null
[36mkafka            |[0m 	ssl.truststore.type = JKS
[36mkafka            |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[36mkafka            |[0m 	transaction.max.timeout.ms = 900000
[36mkafka            |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[36mkafka            |[0m 	transaction.state.log.load.buffer.size = 5242880
[36mkafka            |[0m 	transaction.state.log.min.isr = 2
[36mkafka            |[0m 	transaction.state.log.num.partitions = 50
[36mkafka            |[0m 	transaction.state.log.replication.factor = 3
[36mkafka            |[0m 	transaction.state.log.segment.bytes = 104857600
[36mkafka            |[0m 	transactional.id.expiration.ms = 604800000
[36mkafka            |[0m 	unclean.leader.election.enable = false
[36mkafka            |[0m 	zookeeper.connect = zookeeper:2181
[36mkafka            |[0m 	zookeeper.connection.timeout.ms = null
[36mkafka            |[0m 	zookeeper.max.in.flight.requests = 10
[36mkafka            |[0m 	zookeeper.session.timeout.ms = 6000
[36mkafka            |[0m 	zookeeper.set.acl = false
[36mkafka            |[0m 	zookeeper.sync.time.ms = 2000
[36mkafka            |[0m  (kafka.server.KafkaConfig)
[36mkafka            |[0m [2020-05-28 13:01:40,844] INFO KafkaConfig values: 
[36mkafka            |[0m 	advertised.host.name = null
[36mkafka            |[0m 	advertised.listeners = LISTENER_DOCKER_INTERNAL://kafka:19092,LISTENER_DOCKER_EXTERNAL://kafka:9092
[36mkafka            |[0m 	advertised.port = null
[36mkafka            |[0m 	alter.config.policy.class.name = null
[36mkafka            |[0m 	alter.log.dirs.replication.quota.window.num = 11
[36mkafka            |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[36mkafka            |[0m 	authorizer.class.name = 
[36mkafka            |[0m 	auto.create.topics.enable = true
[36mkafka            |[0m 	auto.leader.rebalance.enable = true
[36mkafka            |[0m 	background.threads = 10
[36mkafka            |[0m 	broker.id = 1
[36mkafka            |[0m 	broker.id.generation.enable = true
[36mkafka            |[0m 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
[36mkafka            |[0m 	broker.rack = null
[36mkafka            |[0m 	client.quota.callback.class = null
[36mkafka            |[0m 	compression.type = producer
[36mkafka            |[0m 	connection.failed.authentication.delay.ms = 100
[36mkafka            |[0m 	connections.max.idle.ms = 600000
[36mkafka            |[0m 	controlled.shutdown.enable = true
[36mkafka            |[0m 	controlled.shutdown.max.retries = 3
[36mkafka            |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[36mkafka            |[0m 	controller.socket.timeout.ms = 30000
[36mkafka            |[0m 	create.topic.policy.class.name = null
[36mkafka            |[0m 	default.replication.factor = 1
[36mkafka            |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[36mkafka            |[0m 	delegation.token.expiry.time.ms = 86400000
[36mkafka            |[0m 	delegation.token.master.key = null
[36mkafka            |[0m 	delegation.token.max.lifetime.ms = 604800000
[36mkafka            |[0m 	delete.records.purgatory.purge.interval.requests = 1
[36mkafka            |[0m 	delete.topic.enable = true
[36mkafka            |[0m 	fetch.purgatory.purge.interval.requests = 1000
[36mkafka            |[0m 	group.initial.rebalance.delay.ms = 3000
[36mkafka            |[0m 	group.max.session.timeout.ms = 300000
[36mkafka            |[0m 	group.min.session.timeout.ms = 6000
[36mkafka            |[0m 	host.name = 
[36mkafka            |[0m 	inter.broker.listener.name = LISTENER_DOCKER_INTERNAL
[36mkafka            |[0m 	inter.broker.protocol.version = 2.1-IV2
[36mkafka            |[0m 	kafka.metrics.polling.interval.secs = 10
[36mkafka            |[0m 	kafka.metrics.reporters = []
[36mkafka            |[0m 	leader.imbalance.check.interval.seconds = 300
[36mkafka            |[0m 	leader.imbalance.per.broker.percentage = 10
[36mkafka            |[0m 	listener.security.protocol.map = LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
[36mkafka            |[0m 	listeners = LISTENER_DOCKER_INTERNAL://0.0.0.0:19092,LISTENER_DOCKER_EXTERNAL://0.0.0.0:9092
[36mkafka            |[0m 	log.cleaner.backoff.ms = 15000
[36mkafka            |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[36mkafka            |[0m 	log.cleaner.delete.retention.ms = 86400000
[36mkafka            |[0m 	log.cleaner.enable = true
[36mkafka            |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[36mkafka            |[0m 	log.cleaner.io.buffer.size = 524288
[36mkafka            |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[36mkafka            |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[36mkafka            |[0m 	log.cleaner.min.compaction.lag.ms = 0
[36mkafka            |[0m 	log.cleaner.threads = 1
[36mkafka            |[0m 	log.cleanup.policy = [delete]
[36mkafka            |[0m 	log.dir = /tmp/kafka-logs
[36mkafka            |[0m 	log.dirs = /var/lib/kafka/data
[36mkafka            |[0m 	log.flush.interval.messages = 9223372036854775807
[36mkafka            |[0m 	log.flush.interval.ms = null
[36mkafka            |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[36mkafka            |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[36mkafka            |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[36mkafka            |[0m 	log.index.interval.bytes = 4096
[36mkafka            |[0m 	log.index.size.max.bytes = 10485760
[36mkafka            |[0m 	log.message.downconversion.enable = true
[36mkafka            |[0m 	log.message.format.version = 2.1-IV2
[36mkafka            |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[36mkafka            |[0m 	log.message.timestamp.type = CreateTime
[36mkafka            |[0m 	log.preallocate = false
[36mkafka            |[0m 	log.retention.bytes = -1
[36mkafka            |[0m 	log.retention.check.interval.ms = 300000
[36mkafka            |[0m 	log.retention.hours = 168
[36mkafka            |[0m 	log.retention.minutes = null
[36mkafka            |[0m 	log.retention.ms = null
[36mkafka            |[0m 	log.roll.hours = 168
[36mkafka            |[0m 	log.roll.jitter.hours = 0
[36mkafka            |[0m 	log.roll.jitter.ms = null
[36mkafka            |[0m 	log.roll.ms = null
[36mkafka            |[0m 	log.segment.bytes = 1073741824
[36mkafka            |[0m 	log.segment.delete.delay.ms = 60000
[36mkafka            |[0m 	max.connections.per.ip = 2147483647
[36mkafka            |[0m 	max.connections.per.ip.overrides = 
[36mkafka            |[0m 	max.incremental.fetch.session.cache.slots = 1000
[36mkafka            |[0m 	message.max.bytes = 1000012
[36mkafka            |[0m 	metric.reporters = []
[36mkafka            |[0m 	metrics.num.samples = 2
[36mkafka            |[0m 	metrics.recording.level = INFO
[36mkafka            |[0m 	metrics.sample.window.ms = 30000
[36mkafka            |[0m 	min.insync.replicas = 1
[36mkafka            |[0m 	num.io.threads = 8
[36mkafka            |[0m 	num.network.threads = 3
[36mkafka            |[0m 	num.partitions = 1
[36mkafka            |[0m 	num.recovery.threads.per.data.dir = 1
[36mkafka            |[0m 	num.replica.alter.log.dirs.threads = null
[36mkafka            |[0m 	num.replica.fetchers = 1
[36mkafka            |[0m 	offset.metadata.max.bytes = 4096
[36mkafka            |[0m 	offsets.commit.required.acks = -1
[36mkafka            |[0m 	offsets.commit.timeout.ms = 5000
[36mkafka            |[0m 	offsets.load.buffer.size = 5242880
[36mkafka            |[0m 	offsets.retention.check.interval.ms = 600000
[36mkafka            |[0m 	offsets.retention.minutes = 10080
[36mkafka            |[0m 	offsets.topic.compression.codec = 0
[36mkafka            |[0m 	offsets.topic.num.partitions = 50
[36mkafka            |[0m 	offsets.topic.replication.factor = 1
[36mkafka            |[0m 	offsets.topic.segment.bytes = 104857600
[36mkafka            |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[36mkafka            |[0m 	password.encoder.iterations = 4096
[36mkafka            |[0m 	password.encoder.key.length = 128
[36mkafka            |[0m 	password.encoder.keyfactory.algorithm = null
[36mkafka            |[0m 	password.encoder.old.secret = null
[36mkafka            |[0m 	password.encoder.secret = null
[36mkafka            |[0m 	port = 9092
[36mkafka            |[0m 	principal.builder.class = null
[36mkafka            |[0m 	producer.purgatory.purge.interval.requests = 1000
[36mkafka            |[0m 	queued.max.request.bytes = -1
[36mkafka            |[0m 	queued.max.requests = 500
[36mkafka            |[0m 	quota.consumer.default = 9223372036854775807
[36mkafka            |[0m 	quota.producer.default = 9223372036854775807
[36mkafka            |[0m 	quota.window.num = 11
[36mkafka            |[0m 	quota.window.size.seconds = 1
[36mkafka            |[0m 	replica.fetch.backoff.ms = 1000
[36mkafka            |[0m 	replica.fetch.max.bytes = 1048576
[36mkafka            |[0m 	replica.fetch.min.bytes = 1
[36mkafka            |[0m 	replica.fetch.response.max.bytes = 10485760
[36mkafka            |[0m 	replica.fetch.wait.max.ms = 500
[36mkafka            |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[36mkafka            |[0m 	replica.lag.time.max.ms = 10000
[36mkafka            |[0m 	replica.socket.receive.buffer.bytes = 65536
[36mkafka            |[0m 	replica.socket.timeout.ms = 30000
[36mkafka            |[0m 	replication.quota.window.num = 11
[36mkafka            |[0m 	replication.quota.window.size.seconds = 1
[36mkafka            |[0m 	request.timeout.ms = 30000
[36mkafka            |[0m 	reserved.broker.max.id = 1000
[36mkafka            |[0m 	sasl.client.callback.handler.class = null
[36mkafka            |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[36mkafka            |[0m 	sasl.jaas.config = null
[36mkafka            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka            |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[36mkafka            |[0m 	sasl.kerberos.service.name = null
[36mkafka            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka            |[0m 	sasl.login.callback.handler.class = null
[36mkafka            |[0m 	sasl.login.class = null
[36mkafka            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka            |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[36mkafka            |[0m 	sasl.server.callback.handler.class = null
[36mkafka            |[0m 	security.inter.broker.protocol = PLAINTEXT
[36mkafka            |[0m 	socket.receive.buffer.bytes = 102400
[36mkafka            |[0m 	socket.request.max.bytes = 104857600
[36mkafka            |[0m 	socket.send.buffer.bytes = 102400
[36mkafka            |[0m 	ssl.cipher.suites = []
[36mkafka            |[0m 	ssl.client.auth = none
[36mkafka            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[36mkafka            |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka            |[0m 	ssl.key.password = null
[36mkafka            |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka            |[0m 	ssl.keystore.location = null
[36mkafka            |[0m 	ssl.keystore.password = null
[36mkafka            |[0m 	ssl.keystore.type = JKS
[36mkafka            |[0m 	ssl.protocol = TLS
[36mkafka            |[0m 	ssl.provider = null
[36mkafka            |[0m 	ssl.secure.random.implementation = null
[36mkafka            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka            |[0m 	ssl.truststore.location = null
[36mkafka            |[0m 	ssl.truststore.password = null
[36mkafka            |[0m 	ssl.truststore.type = JKS
[36mkafka            |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[36mkafka            |[0m 	transaction.max.timeout.ms = 900000
[36mkafka            |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[36mkafka            |[0m 	transaction.state.log.load.buffer.size = 5242880
[36mkafka            |[0m 	transaction.state.log.min.isr = 2
[36mkafka            |[0m 	transaction.state.log.num.partitions = 50
[36mkafka            |[0m 	transaction.state.log.replication.factor = 3
[36mkafka            |[0m 	transaction.state.log.segment.bytes = 104857600
[36mkafka            |[0m 	transactional.id.expiration.ms = 604800000
[36mkafka            |[0m 	unclean.leader.election.enable = false
[36mkafka            |[0m 	zookeeper.connect = zookeeper:2181
[36mkafka            |[0m 	zookeeper.connection.timeout.ms = null
[36mkafka            |[0m 	zookeeper.max.in.flight.requests = 10
[36mkafka            |[0m 	zookeeper.session.timeout.ms = 6000
[36mkafka            |[0m 	zookeeper.set.acl = false
[36mkafka            |[0m 	zookeeper.sync.time.ms = 2000
[36mkafka            |[0m  (kafka.server.KafkaConfig)
[36mkafka            |[0m [2020-05-28 13:01:40,966] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka            |[0m [2020-05-28 13:01:40,970] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka            |[0m [2020-05-28 13:01:40,972] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka            |[0m [2020-05-28 13:01:41,097] INFO Loading logs. (kafka.log.LogManager)
[36mkafka            |[0m [2020-05-28 13:01:41,228] INFO [Log partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,250] INFO [Log partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 98 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,285] INFO [Log partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,293] INFO [Log partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,314] INFO [Log partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,315] INFO [Log partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,334] INFO [Log partition=ingredientsReq-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,335] INFO [Log partition=ingredientsReq-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,349] INFO [Log partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,350] INFO [Log partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,369] INFO [Log partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,369] INFO [Log partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,391] INFO [Log partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,392] INFO [Log partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,482] INFO [Log partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,483] INFO [Log partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 41 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,500] INFO [Log partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,500] INFO [Log partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,526] INFO [Log partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,527] INFO [Log partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,547] INFO [Log partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,548] INFO [Log partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,569] INFO [Log partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,569] INFO [Log partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,590] INFO [Log partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,590] INFO [Log partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,609] INFO [Log partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,610] INFO [Log partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,622] INFO [Log partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,624] INFO [Log partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,648] INFO [Log partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,649] INFO [Log partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,677] INFO [Log partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,677] INFO [Log partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,727] INFO [Log partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 4549 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,747] INFO [ProducerStateManager partition=__consumer_offsets-9] Loading producer state from snapshot file '/var/lib/kafka/data/__consumer_offsets-9/00000000000000004549.snapshot' (kafka.log.ProducerStateManager)
[36mkafka            |[0m [2020-05-28 13:01:41,776] INFO [Log partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Completed load of log with 2 segments, log start offset 0 and log end offset 4549 in 86 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,789] INFO [Log partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,789] INFO [Log partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,806] INFO [Log partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,807] INFO [Log partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,830] INFO [Log partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,831] INFO [Log partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,842] INFO [Log partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,843] INFO [Log partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,862] INFO [Log partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,863] INFO [Log partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,899] INFO [Log partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,901] INFO [Log partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,917] INFO [Log partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,918] INFO [Log partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,930] INFO [Log partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,930] INFO [Log partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,946] INFO [Log partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,946] INFO [Log partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,958] INFO [Log partition=instruments-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,960] INFO [Log partition=instruments-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,974] INFO [Log partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,975] INFO [Log partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,992] INFO [Log partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:41,993] INFO [Log partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,004] INFO [Log partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,005] INFO [Log partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,021] INFO [Log partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,022] INFO [Log partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,035] INFO [Log partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,036] INFO [Log partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,055] INFO [Log partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,056] INFO [Log partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,072] INFO [Log partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,073] INFO [Log partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,084] INFO [Log partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,085] INFO [Log partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,098] INFO [Log partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,099] INFO [Log partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,109] INFO [Log partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,110] INFO [Log partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,120] INFO [Log partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,121] INFO [Log partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,134] INFO [Log partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,134] INFO [Log partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,147] INFO [Log partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,147] INFO [Log partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,160] INFO [Log partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,161] INFO [Log partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,176] INFO [Log partition=__confluent.support.metrics-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,176] INFO [Log partition=__confluent.support.metrics-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,189] INFO [Log partition=usersReq-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,190] INFO [Log partition=usersReq-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,224] INFO [Log partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,225] INFO [Log partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,238] INFO [Log partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,239] INFO [Log partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,252] INFO [Log partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,253] INFO [Log partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,272] INFO [Log partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,273] INFO [Log partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,290] INFO [Log partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,291] INFO [Log partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,307] INFO [Log partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,308] INFO [Log partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,320] INFO [Log partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,322] INFO [Log partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,345] INFO [Log partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,346] INFO [Log partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,371] INFO [Log partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,381] INFO [Log partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 32 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,410] INFO [Log partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,411] INFO [Log partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 26 ms (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:42,417] INFO Logs loading complete in 1318 ms. (kafka.log.LogManager)
[36mkafka            |[0m [2020-05-28 13:01:42,452] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[36mkafka            |[0m [2020-05-28 13:01:42,454] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[36mkafka            |[0m [2020-05-28 13:01:42,457] INFO Starting the log cleaner (kafka.log.LogCleaner)
[36mkafka            |[0m [2020-05-28 13:01:42,685] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[36mkafka            |[0m [2020-05-28 13:01:43,466] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.Acceptor)
[36mkafka            |[0m [2020-05-28 13:01:43,607] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[36mkafka            |[0m [2020-05-28 13:01:43,682] INFO [SocketServer brokerId=1] Started 2 acceptor threads (kafka.network.SocketServer)
[36mkafka            |[0m [2020-05-28 13:01:43,773] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka            |[0m [2020-05-28 13:01:43,787] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka            |[0m [2020-05-28 13:01:43,791] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka            |[0m [2020-05-28 13:01:43,870] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[36mkafka            |[0m [2020-05-28 13:01:44,103] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
[36mkafka            |[0m [2020-05-28 13:01:44,121] INFO Result of znode creation at /brokers/ids/1 is: OK (kafka.zk.KafkaZkClient)
[36mkafka            |[0m [2020-05-28 13:01:44,124] INFO Registered broker 1 at path /brokers/ids/1 with addresses: ArrayBuffer(EndPoint(kafka,19092,ListenerName(LISTENER_DOCKER_INTERNAL),PLAINTEXT), EndPoint(kafka,9092,ListenerName(LISTENER_DOCKER_EXTERNAL),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[36mkafka            |[0m [2020-05-28 13:01:44,327] INFO [ControllerEventThread controllerId=1] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
[36mkafka            |[0m [2020-05-28 13:01:44,341] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka            |[0m [2020-05-28 13:01:44,364] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka            |[0m [2020-05-28 13:01:44,364] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka            |[0m [2020-05-28 13:01:44,541] INFO [Controller id=1] 1 successfully elected as the controller. Epoch incremented to 41 and epoch zk version is now 41 (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:44,541] INFO [Controller id=1] Registering handlers (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:44,551] INFO [Controller id=1] Deleting log dir event notifications (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:44,557] INFO [Controller id=1] Deleting isr change notifications (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:44,566] INFO [Controller id=1] Initializing controller context (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:44,599] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[36mkafka            |[0m [2020-05-28 13:01:44,603] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[36mkafka            |[0m [2020-05-28 13:01:44,616] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 11 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:44,672] INFO [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:40000,blockEndProducerId:40999) by writing to Zk with path version 41 (kafka.coordinator.transaction.ProducerIdManager)
[36mkafka            |[0m [2020-05-28 13:01:44,883] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[36mkafka            |[0m [2020-05-28 13:01:44,895] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[36mkafka            |[0m [2020-05-28 13:01:44,924] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[36mkafka            |[0m [2020-05-28 13:01:45,038] INFO [RequestSendThread controllerId=1] Starting (kafka.controller.RequestSendThread)
[36mkafka            |[0m [2020-05-28 13:01:45,047] INFO [Controller id=1] Partitions being reassigned: Map() (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,048] INFO [Controller id=1] Currently active brokers in the cluster: Set(1) (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,049] INFO [Controller id=1] Currently shutting brokers in the cluster: Set() (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,062] INFO [Controller id=1] Current list of topics in the cluster: Set(usersReq, __consumer_offsets, ingredientsReq, __confluent.support.metrics, instruments) (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,063] INFO [Controller id=1] Fetching topic deletions in progress (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,088] INFO [Controller id=1] List of topics to be deleted:  (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,090] INFO [Controller id=1] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,093] INFO [Controller id=1] Initializing topic deletion manager (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,098] INFO [Controller id=1] Sending update metadata request (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,185] INFO [ReplicaStateMachine controllerId=1] Initializing replica state (kafka.controller.ReplicaStateMachine)
[36mkafka            |[0m [2020-05-28 13:01:45,199] INFO [RequestSendThread controllerId=1] Controller 1 connected to kafka:19092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
[36mkafka            |[0m [2020-05-28 13:01:45,221] INFO [ReplicaStateMachine controllerId=1] Triggering online replica state changes (kafka.controller.ReplicaStateMachine)
[36mkafka            |[0m [2020-05-28 13:01:45,298] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[36mkafka            |[0m [2020-05-28 13:01:45,381] WARN [Log partition=__confluent.support.metrics-0, dir=/var/lib/kafka/data] retention.ms for topic __confluent.support.metrics is set to 31536000000. It is smaller than message.timestamp.difference.max.ms's value 9223372036854775807. This may result in frequent log rolling. (kafka.log.Log)
[36mkafka            |[0m [2020-05-28 13:01:45,506] INFO [SocketServer brokerId=1] Started processors for 2 acceptors (kafka.network.SocketServer)
[36mkafka            |[0m [2020-05-28 13:01:45,511] INFO [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map([Topic=__consumer_offsets,Partition=30,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=8,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=4,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=26,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=18,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=36,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=13,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=10,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=41,Replica=1] -> OnlineReplica, [Topic=__confluent.support.metrics,Partition=0,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=21,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=3,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=49,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=33,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=0,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=32,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=37,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=15,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=22,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=11,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=24,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=9,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=29,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=46,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=35,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=45,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=19,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=7,Replica=1] -> OnlineReplica, [Topic=usersReq,Partition=0,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=27,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=14,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=28,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=44,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=40,Replica=1] -> OnlineReplica, [Topic=instruments,Partition=0,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=43,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=34,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=6,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=31,Replica=1] -> OnlineReplica, [Topic=ingredientsReq,Partition=0,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=47,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=1,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=17,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=20,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=16,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=48,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=12,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=2,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=38,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=42,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=39,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=5,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=25,Replica=1] -> OnlineReplica, [Topic=__consumer_offsets,Partition=23,Replica=1] -> OnlineReplica) (kafka.controller.ReplicaStateMachine)
[36mkafka            |[0m [2020-05-28 13:01:45,514] INFO [PartitionStateMachine controllerId=1] Initializing partition state (kafka.controller.PartitionStateMachine)
[36mkafka            |[0m [2020-05-28 13:01:45,516] INFO Kafka version : 2.1.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka            |[0m [2020-05-28 13:01:45,520] INFO Kafka commitId : bda8715f42a1a3db (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka            |[0m [2020-05-28 13:01:45,522] INFO [PartitionStateMachine controllerId=1] Triggering online partition state changes (kafka.controller.PartitionStateMachine)
[36mkafka            |[0m [2020-05-28 13:01:45,538] INFO [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map(__consumer_offsets-49 -> OnlinePartition, __consumer_offsets-38 -> OnlinePartition, __consumer_offsets-16 -> OnlinePartition, __consumer_offsets-27 -> OnlinePartition, __consumer_offsets-8 -> OnlinePartition, __consumer_offsets-19 -> OnlinePartition, __consumer_offsets-13 -> OnlinePartition, __consumer_offsets-2 -> OnlinePartition, __consumer_offsets-46 -> OnlinePartition, __consumer_offsets-35 -> OnlinePartition, __consumer_offsets-24 -> OnlinePartition, __consumer_offsets-5 -> OnlinePartition, usersReq-0 -> OnlinePartition, __consumer_offsets-43 -> OnlinePartition, __consumer_offsets-32 -> OnlinePartition, __consumer_offsets-21 -> OnlinePartition, __consumer_offsets-10 -> OnlinePartition, __consumer_offsets-37 -> OnlinePartition, __consumer_offsets-48 -> OnlinePartition, instruments-0 -> OnlinePartition, __consumer_offsets-40 -> OnlinePartition, __consumer_offsets-29 -> OnlinePartition, __consumer_offsets-18 -> OnlinePartition, __consumer_offsets-7 -> OnlinePartition, __consumer_offsets-34 -> OnlinePartition, __consumer_offsets-23 -> OnlinePartition, __consumer_offsets-45 -> OnlinePartition, __consumer_offsets-26 -> OnlinePartition, __consumer_offsets-4 -> OnlinePartition, __consumer_offsets-15 -> OnlinePartition, __consumer_offsets-42 -> OnlinePartition, __consumer_offsets-31 -> OnlinePartition, __consumer_offsets-9 -> OnlinePartition, __consumer_offsets-20 -> OnlinePartition, __consumer_offsets-12 -> OnlinePartition, __consumer_offsets-1 -> OnlinePartition, __confluent.support.metrics-0 -> OnlinePartition, ingredientsReq-0 -> OnlinePartition, __consumer_offsets-28 -> OnlinePartition, __consumer_offsets-17 -> OnlinePartition, __consumer_offsets-6 -> OnlinePartition, __consumer_offsets-39 -> OnlinePartition, __consumer_offsets-44 -> OnlinePartition, __consumer_offsets-36 -> OnlinePartition, __consumer_offsets-47 -> OnlinePartition, __consumer_offsets-3 -> OnlinePartition, __consumer_offsets-25 -> OnlinePartition, __consumer_offsets-14 -> OnlinePartition, __consumer_offsets-41 -> OnlinePartition, __consumer_offsets-30 -> OnlinePartition, __consumer_offsets-33 -> OnlinePartition, __consumer_offsets-22 -> OnlinePartition, __consumer_offsets-11 -> OnlinePartition, __consumer_offsets-0 -> OnlinePartition) (kafka.controller.PartitionStateMachine)
[36mkafka            |[0m [2020-05-28 13:01:45,538] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
[36mkafka            |[0m [2020-05-28 13:01:45,546] INFO [Controller id=1] Ready to serve as the new controller with epoch 41 (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,563] INFO Waiting until monitored service is ready for metrics collection (io.confluent.support.metrics.BaseMetricsReporter)
[36mkafka            |[0m [2020-05-28 13:01:45,607] INFO [Controller id=1] Removing partitions Set() from the list of reassigned partitions in zookeeper (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,608] INFO [Controller id=1] No more partitions need to be reassigned. Deleting zk path /admin/reassign_partitions (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,613] INFO Monitored service is now ready (io.confluent.support.metrics.BaseMetricsReporter)
[36mkafka            |[0m [2020-05-28 13:01:45,613] INFO Attempting to collect and submit metrics (io.confluent.support.metrics.BaseMetricsReporter)
[35mzookeeper        |[0m 2020-05-28 13:01:45,657 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@596] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:multi cxid:0x71 zxid:0x412 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/reassign_partitions Error:KeeperErrorCode = NoNode for /admin/reassign_partitions
[36mkafka            |[0m [2020-05-28 13:01:45,676] INFO [Controller id=1] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,681] INFO [Controller id=1] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,682] INFO [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,685] INFO [Controller id=1] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,688] INFO [Controller id=1] Starting preferred replica leader election for partitions  (kafka.controller.KafkaController)
[35mzookeeper        |[0m 2020-05-28 13:01:45,697 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@596] - Got user-level KeeperException when processing sessionid:0x1725b5f6bbe0001 type:multi cxid:0x73 zxid:0x413 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
[36mkafka            |[0m [2020-05-28 13:01:45,705] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
[36mkafka            |[0m [2020-05-28 13:01:45,944] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, instruments-0, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, usersReq-0, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, ingredientsReq-0, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __confluent.support.metrics-0, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[36mkafka            |[0m [2020-05-28 13:01:46,029] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,052] INFO [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,124] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,132] INFO [Partition __consumer_offsets-29 broker=1] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,159] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,161] INFO [Partition __consumer_offsets-48 broker=1] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,203] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,205] INFO [Partition __consumer_offsets-10 broker=1] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,239] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,241] INFO [Partition __consumer_offsets-45 broker=1] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,279] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,280] INFO [Partition __consumer_offsets-26 broker=1] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,300] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,307] INFO [Partition __consumer_offsets-7 broker=1] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,332] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,333] INFO [Partition __consumer_offsets-42 broker=1] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,357] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,362] INFO [Partition __consumer_offsets-4 broker=1] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,382] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,385] INFO [Partition __consumer_offsets-23 broker=1] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,399] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,402] INFO [Partition __consumer_offsets-1 broker=1] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,426] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,428] INFO [Partition __consumer_offsets-39 broker=1] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,447] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,448] INFO [Partition __consumer_offsets-20 broker=1] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,468] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,473] INFO [Partition __consumer_offsets-17 broker=1] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,495] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,500] INFO [Partition __consumer_offsets-36 broker=1] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,521] INFO Replica loaded for partition usersReq-0 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,524] INFO [Partition usersReq-0 broker=1] usersReq-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,546] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,559] INFO [Partition __consumer_offsets-14 broker=1] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,623] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,625] INFO [Partition __consumer_offsets-33 broker=1] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,645] INFO Replica loaded for partition instruments-0 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,655] INFO [Partition instruments-0 broker=1] instruments-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,688] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,689] INFO [Partition __consumer_offsets-49 broker=1] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,895] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,895] INFO [Partition __consumer_offsets-11 broker=1] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,921] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,925] INFO [Partition __consumer_offsets-30 broker=1] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,949] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,949] INFO [Partition __consumer_offsets-46 broker=1] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,967] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,967] INFO [Partition __consumer_offsets-27 broker=1] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:46,978] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,979] INFO [Partition __consumer_offsets-8 broker=1] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32muser-service     |[0m [0m[0m2020-05-28 13:01:46,991 INFO  [org.wildfly.swarm.datasources] (main) THORN1003: Auto-detected JDBC driver for h2
[36mkafka            |[0m [2020-05-28 13:01:46,996] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:46,997] INFO [Partition __consumer_offsets-24 broker=1] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,018] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,019] INFO [Partition __consumer_offsets-43 broker=1] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,046] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,047] INFO [Partition __consumer_offsets-5 broker=1] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,066] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,068] INFO [Partition __consumer_offsets-21 broker=1] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,097] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,097] INFO [Partition __consumer_offsets-2 broker=1] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,138] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,138] INFO [Partition __consumer_offsets-40 broker=1] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,157] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,162] INFO [Partition __consumer_offsets-37 broker=1] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,186] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,187] INFO [Partition __consumer_offsets-18 broker=1] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,226] INFO Replica loaded for partition __confluent.support.metrics-0 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,227] INFO [Partition __confluent.support.metrics-0 broker=1] __confluent.support.metrics-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,269] WARN The replication factor of topic __confluent.support.metrics is 1, which is less than the desired replication factor of 3.  If you happen to add more brokers to this cluster, then it is important to increase the replication factor of the topic to eventually 3 to ensure reliable and durable metrics collection. (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[36mkafka            |[0m [2020-05-28 13:01:47,272] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,290] INFO [Partition __consumer_offsets-34 broker=1] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,305] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,311] INFO [Partition __consumer_offsets-15 broker=1] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,347] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[32muser-service     |[0m [0m[0m2020-05-28 13:01:47,352 INFO  [org.wildfly.swarm.datasources] (main) THORN1003: Auto-detected JDBC driver for postgresql
[36mkafka            |[0m [2020-05-28 13:01:47,350] INFO ProducerConfig values: 
[36mkafka            |[0m 	acks = 1
[36mkafka            |[0m 	batch.size = 16384
[36mkafka            |[0m 	bootstrap.servers = []
[36mkafka            |[0m 	buffer.memory = 33554432
[36mkafka            |[0m 	client.dns.lookup = default
[36mkafka            |[0m 	client.id = 
[36mkafka            |[0m 	compression.type = none
[36mkafka            |[0m 	connections.max.idle.ms = 540000
[36mkafka            |[0m 	delivery.timeout.ms = 120000
[36mkafka            |[0m 	enable.idempotence = false
[36mkafka            |[0m 	interceptor.classes = []
[36mkafka            |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mkafka            |[0m 	linger.ms = 0
[36mkafka            |[0m 	max.block.ms = 10000
[36mkafka            |[0m 	max.in.flight.requests.per.connection = 5
[36mkafka            |[0m 	max.request.size = 1048576
[36mkafka            |[0m 	metadata.max.age.ms = 300000
[36mkafka            |[0m 	metric.reporters = []
[36mkafka            |[0m 	metrics.num.samples = 2
[36mkafka            |[0m 	metrics.recording.level = INFO
[36mkafka            |[0m 	metrics.sample.window.ms = 30000
[36mkafka            |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mkafka            |[0m 	receive.buffer.bytes = 32768
[36mkafka            |[0m 	reconnect.backoff.max.ms = 1000
[36mkafka            |[0m 	reconnect.backoff.ms = 50
[36mkafka            |[0m 	request.timeout.ms = 30000
[36mkafka            |[0m 	retries = 2147483647
[36mkafka            |[0m 	retry.backoff.ms = 100
[36mkafka            |[0m 	sasl.client.callback.handler.class = null
[36mkafka            |[0m 	sasl.jaas.config = null
[36mkafka            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka            |[0m 	sasl.kerberos.service.name = null
[36mkafka            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka            |[0m 	sasl.login.callback.handler.class = null
[36mkafka            |[0m 	sasl.login.class = null
[36mkafka            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka            |[0m 	sasl.mechanism = GSSAPI
[36mkafka            |[0m 	security.protocol = PLAINTEXT
[36mkafka            |[0m 	send.buffer.bytes = 131072
[36mkafka            |[0m 	ssl.cipher.suites = null
[36mkafka            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[36mkafka            |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka            |[0m 	ssl.key.password = null
[36mkafka            |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka            |[0m 	ssl.keystore.location = null
[36mkafka            |[0m 	ssl.keystore.password = null
[36mkafka            |[0m 	ssl.keystore.type = JKS
[36mkafka            |[0m 	ssl.protocol = TLS
[36mkafka            |[0m 	ssl.provider = null
[36mkafka            |[0m 	ssl.secure.random.implementation = null
[36mkafka            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka            |[0m 	ssl.truststore.location = null
[36mkafka            |[0m 	ssl.truststore.password = null
[36mkafka            |[0m 	ssl.truststore.type = JKS
[36mkafka            |[0m 	transaction.timeout.ms = 60000
[36mkafka            |[0m 	transactional.id = null
[36mkafka            |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mkafka            |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mkafka            |[0m [2020-05-28 13:01:47,355] INFO [Partition __consumer_offsets-12 broker=1] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,391] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,394] INFO [Partition __consumer_offsets-31 broker=1] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,405] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[36mkafka            |[0m [2020-05-28 13:01:47,414] ERROR Could not submit metrics to Kafka topic __confluent.support.metrics: Failed to construct kafka producer (io.confluent.support.metrics.BaseMetricsReporter)
[36mkafka            |[0m [2020-05-28 13:01:47,441] INFO Replica loaded for partition ingredientsReq-0 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,441] INFO [Partition ingredientsReq-0 broker=1] ingredientsReq-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,491] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 4549 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,493] INFO [Partition __consumer_offsets-9 broker=1] __consumer_offsets-9 starts at Leader Epoch 0 from offset 4549. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,512] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,537] INFO [Partition __consumer_offsets-47 broker=1] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,567] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,567] INFO [Partition __consumer_offsets-19 broker=1] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,587] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,602] INFO [Partition __consumer_offsets-28 broker=1] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,634] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,637] INFO [Partition __consumer_offsets-38 broker=1] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,653] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,662] INFO [Partition __consumer_offsets-35 broker=1] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32muser-service     |[0m [0m[0m2020-05-28 13:01:47,683 INFO  [org.wildfly.swarm.jmx] (main) JMX not configured for remote access
[36mkafka            |[0m [2020-05-28 13:01:47,686] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,687] INFO [Partition __consumer_offsets-6 broker=1] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,704] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,706] INFO [Partition __consumer_offsets-44 broker=1] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,723] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,736] INFO [Partition __consumer_offsets-25 broker=1] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,756] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,760] INFO [Partition __consumer_offsets-16 broker=1] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,784] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,787] INFO [Partition __consumer_offsets-22 broker=1] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,813] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,814] INFO [Partition __consumer_offsets-41 broker=1] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,843] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,846] INFO [Partition __consumer_offsets-32 broker=1] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,861] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,869] INFO [Partition __consumer_offsets-3 broker=1] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,894] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[36mkafka            |[0m [2020-05-28 13:01:47,903] INFO [Partition __consumer_offsets-13 broker=1] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36mkafka            |[0m [2020-05-28 13:01:47,983] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,010] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,012] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,012] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,014] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,016] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,017] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,018] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,018] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,019] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,019] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,020] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,020] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,021] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,025] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,026] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,027] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,027] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,028] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,030] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,031] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,032] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,033] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,035] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,035] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,036] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,038] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,040] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,040] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,041] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,042] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,042] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,047] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,048] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,049] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,050] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,051] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,052] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,054] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,054] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,055] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,055] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,055] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,055] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,056] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,057] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,058] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,045] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 42 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,060] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,068] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,070] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,071] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,074] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,084] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 6 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,128] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,133] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,136] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,139] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,140] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,142] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,143] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,144] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,147] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,148] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,149] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,149] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,150] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,151] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,152] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,153] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,160] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,188] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 23 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,189] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,190] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,191] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,191] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,192] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,192] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,193] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,194] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,196] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,199] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,201] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,202] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,883] INFO [GroupCoordinator 1]: Loading group metadata for pinfo-microservices with generation 61 (kafka.coordinator.group.GroupCoordinator)
[36mkafka            |[0m [2020-05-28 13:01:48,912] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 709 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,919] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,920] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,921] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,922] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,923] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,924] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,925] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,926] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,926] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,927] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,930] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,934] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka            |[0m [2020-05-28 13:01:48,935] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32muser-service     |[0m [0m[0m2020-05-28 13:01:50,477 INFO  [org.jboss.msc] (main) JBoss MSC version 1.4.11.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:50,504 INFO  [org.jboss.threads] (main) JBoss Threads version 2.3.3.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:51,630 INFO  [org.jboss.as] (MSC service thread 1-3) WFLYSRV0049: Thorntail 2.6.0.Final (WildFly Core 10.0.3.Final) starting
[36mkafka            |[0m [2020-05-28 13:01:51,778] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[32muser-service     |[0m [0m[0m2020-05-28 13:01:51,803 INFO  [org.wildfly.swarm] (MSC service thread 1-3) THORN0019: Install MSC service for command line args: []
[32muser-service     |[0m [0m[0m2020-05-28 13:01:54,074 INFO  [org.wildfly.security] (ServerService Thread Pool -- 11) ELY00001: WildFly Elytron version 1.10.4.Final
[32muser-service     |[0m [0m[33m2020-05-28 13:01:55,767 WARN  [org.jboss.as.txn] (ServerService Thread Pool -- 32) WFLYTX0013: The node-identifier attribute on the /subsystem=transactions is set to the default value. This is a danger for environments running multiple servers. Please make sure the attribute value is unique.
[32muser-service     |[0m [0m[0m2020-05-28 13:01:55,882 INFO  [org.jboss.as.naming] (ServerService Thread Pool -- 25) WFLYNAM0001: Activating Naming Subsystem
[32muser-service     |[0m [0m[0m2020-05-28 13:01:55,950 INFO  [org.jboss.as.clustering.infinispan] (ServerService Thread Pool -- 31) WFLYCLINF0001: Activating Infinispan subsystem.
[32muser-service     |[0m [0m[0m2020-05-28 13:01:55,966 INFO  [org.jboss.as.security] (ServerService Thread Pool -- 33) WFLYSEC0002: Activating Security Subsystem
[32muser-service     |[0m [0m[0m2020-05-28 13:01:55,969 INFO  [org.jboss.as.security] (MSC service thread 1-2) WFLYSEC0001: Current PicketBox version=5.0.3.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:56,082 INFO  [org.jboss.as.jaxrs] (ServerService Thread Pool -- 22) WFLYRS0016: RESTEasy version 3.9.1.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:56,116 INFO  [org.xnio] (ServerService Thread Pool -- 29) XNIO version 3.7.3.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:56,182 INFO  [org.jboss.as.connector.subsystems.datasources] (ServerService Thread Pool -- 20) WFLYJCA0004: Deploying JDBC-compliant driver class org.h2.Driver (version 1.4)
[32muser-service     |[0m [0m[0m2020-05-28 13:01:56,204 INFO  [org.xnio.nio] (ServerService Thread Pool -- 29) XNIO NIO Implementation Version 3.7.3.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:56,239 INFO  [org.jboss.as.connector] (MSC service thread 1-1) WFLYJCA0009: Starting JCA Subsystem (WildFly/IronJacamar 1.4.17.Final)
[32muser-service     |[0m [0m[0m2020-05-28 13:01:56,290 INFO  [org.jboss.as.connector.deployers.jdbc] (MSC service thread 1-7) WFLYJCA0018: Started Driver service with driver-name = myh2
[32muser-service     |[0m [0m[0m2020-05-28 13:01:56,380 INFO  [org.jboss.as.naming] (MSC service thread 1-1) WFLYNAM0003: Starting Naming Service
[32muser-service     |[0m [0m[0m2020-05-28 13:01:57,244 INFO  [org.jboss.as.connector.subsystems.datasources] (ServerService Thread Pool -- 20) WFLYJCA0005: Deploying non-JDBC-compliant driver class org.postgresql.Driver (version 42.2)
[32muser-service     |[0m [0m[0m2020-05-28 13:01:57,254 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-7) WFLYUT0003: Undertow 2.0.27.Final starting
[32muser-service     |[0m [0m[0m2020-05-28 13:01:57,271 INFO  [org.jboss.as.connector.deployers.jdbc] (MSC service thread 1-5) WFLYJCA0018: Started Driver service with driver-name = postgresql
[32muser-service     |[0m [0m[0m2020-05-28 13:01:57,313 INFO  [org.jboss.as.connector.subsystems.datasources] (ServerService Thread Pool -- 20) WFLYJCA0004: Deploying JDBC-compliant driver class org.h2.Driver (version 1.4)
[32muser-service     |[0m [0m[0m2020-05-28 13:01:57,370 INFO  [org.jboss.as.connector.deployers.jdbc] (MSC service thread 1-3) WFLYJCA0018: Started Driver service with driver-name = h2
[32muser-service     |[0m [0m[0m2020-05-28 13:01:57,438 INFO  [org.wildfly.extension.io] (ServerService Thread Pool -- 29) WFLYIO001: Worker 'default' has auto-configured to 8 core threads with 64 task threads based on your 4 available processors
[32muser-service     |[0m [0m[0m2020-05-28 13:01:57,990 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-6) WFLYUT0012: Started server default-server.
[32muser-service     |[0m [0m[0m2020-05-28 13:01:58,027 INFO  [org.jboss.remoting] (MSC service thread 1-7) JBoss Remoting version 5.0.15.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:01:58,622 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-5) WFLYUT0006: Undertow HTTP listener default listening on 0.0.0.0:28080
[32muser-service     |[0m [0m[33m2020-05-28 13:01:58,687 WARN  [org.jboss.as.remoting] (MSC service thread 1-8) ****** All authentication is ANONYMOUS for org.jboss.as.remoting.RemotingHttpUpgradeService
[36mkafka            |[0m [2020-05-28 13:01:58,920] INFO [GroupCoordinator 1]: Member consumer-1-cd3c6573-0128-46ae-9d80-47d0252f9da5 in group pinfo-microservices has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[36mkafka            |[0m [2020-05-28 13:01:58,928] INFO [GroupCoordinator 1]: Preparing to rebalance group pinfo-microservices in state PreparingRebalance with old generation 61 (__consumer_offsets-9) (reason: removing member consumer-1-cd3c6573-0128-46ae-9d80-47d0252f9da5 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[36mkafka            |[0m [2020-05-28 13:01:58,935] INFO [GroupCoordinator 1]: Group pinfo-microservices with generation 62 is now empty (__consumer_offsets-9) (kafka.coordinator.group.GroupCoordinator)
[32muser-service     |[0m [0m[0m2020-05-28 13:01:59,712 INFO  [org.jboss.as.connector.subsystems.datasources] (MSC service thread 1-1) WFLYJCA0001: Bound data source [java:jboss/datasources/auserds]
[32muser-service     |[0m [0m[0m2020-05-28 13:01:59,930 INFO  [org.infinispan.factories.GlobalComponentRegistry] (MSC service thread 1-2) ISPN000128: Infinispan version: Infinispan 'Infinity Minus ONE +2' 9.4.16.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:02:01,358 INFO  [org.jboss.as.clustering.infinispan] (ServerService Thread Pool -- 38) WFLYCLINF0002: Started passivation cache from web container
[32muser-service     |[0m [0m[0m2020-05-28 13:02:01,372 INFO  [org.jboss.as.clustering.infinispan] (ServerService Thread Pool -- 37) WFLYCLINF0002: Started default cache from server container
[32muser-service     |[0m [0m[0m2020-05-28 13:02:01,866 INFO  [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0212: Resuming server
[32muser-service     |[0m [0m[0m2020-05-28 13:02:01,870 INFO  [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: Thorntail 2.6.0.Final (WildFly Core 10.0.3.Final) started in 11610ms - Started 255 of 389 services (241 services are lazy, passive or on-demand)
[32muser-service     |[0m [0m[0m2020-05-28 13:02:05,464 INFO  [org.wildfly.swarm.swagger] (main) TTSWGR0004: Configure Swagger for deployment user-service-0.4.0-SNAPSHOT.war with package api.rest
[32muser-service     |[0m [0m[0m2020-05-28 13:02:09,149 INFO  [org.wildfly.swarm.runtime.deployer] (main) deploying user-service-0.4.0-SNAPSHOT.war
[32muser-service     |[0m [0m[0m2020-05-28 13:02:09,251 INFO  [org.jboss.as.server.deployment] (MSC service thread 1-3) WFLYSRV0027: Starting deployment of "user-service-0.4.0-SNAPSHOT.war" (runtime-name: "user-service-0.4.0-SNAPSHOT.war")
[32muser-service     |[0m [0m[0m2020-05-28 13:02:13,111 INFO  [org.jboss.as.jpa] (MSC service thread 1-1) WFLYJPA0002: Read persistence.xml for AUserPU
[32muser-service     |[0m [0m[33m2020-05-28 13:02:13,384 WARN  [org.jboss.as.dependency.private] (MSC service thread 1-2) WFLYSRV0018: Deployment "deployment.user-service-0.4.0-SNAPSHOT.war" is using a private module ("org.jboss.ironjacamar.jdbcadapters") which may be changed or removed in future versions without notice.
[32muser-service     |[0m [0m[33m2020-05-28 13:02:13,387 WARN  [org.jboss.as.dependency.private] (MSC service thread 1-2) WFLYSRV0018: Deployment "deployment.user-service-0.4.0-SNAPSHOT.war" is using a private module ("org.infinispan") which may be changed or removed in future versions without notice.
[32muser-service     |[0m [0m[33m2020-05-28 13:02:13,389 WARN  [org.jboss.as.dependency.private] (MSC service thread 1-2) WFLYSRV0018: Deployment "deployment.user-service-0.4.0-SNAPSHOT.war" is using a private module ("org.infinispan.commons") which may be changed or removed in future versions without notice.
[32muser-service     |[0m [0m[33m2020-05-28 13:02:13,389 WARN  [org.jboss.as.dependency.private] (MSC service thread 1-2) WFLYSRV0018: Deployment "deployment.user-service-0.4.0-SNAPSHOT.war" is using a private module ("org.jboss.jts") which may be changed or removed in future versions without notice.
[32muser-service     |[0m [0m[0m2020-05-28 13:02:13,459 INFO  [org.jboss.as.jpa] (ServerService Thread Pool -- 8) WFLYJPA0010: Starting Persistence Unit (phase 1 of 2) Service 'user-service-0.4.0-SNAPSHOT.war#AUserPU'
[32muser-service     |[0m [0m[0m2020-05-28 13:02:13,545 INFO  [org.hibernate.jpa.internal.util.LogHelper] (ServerService Thread Pool -- 8) HHH000204: Processing PersistenceUnitInfo [
[32muser-service     |[0m 	name: AUserPU
[32muser-service     |[0m 	...]
[32muser-service     |[0m [0m[0m2020-05-28 13:02:13,546 INFO  [org.jboss.weld.deployer] (MSC service thread 1-8) WFLYWELD0003: Processing weld deployment user-service-0.4.0-SNAPSHOT.war
[32muser-service     |[0m [0m[0m2020-05-28 13:02:13,818 INFO  [org.hibernate.validator.internal.util.Version] (MSC service thread 1-8) HV000001: Hibernate Validator 6.0.18.Final
[32muser-service     |[0m [0m[0m2020-05-28 13:02:13,825 INFO  [org.hibernate.Version] (ServerService Thread Pool -- 8) HHH000412: Hibernate Core {5.3.13.Final}
[32muser-service     |[0m [0m[0m2020-05-28 13:02:13,840 INFO  [org.hibernate.cfg.Environment] (ServerService Thread Pool -- 8) HHH000206: hibernate.properties not found
[32muser-service     |[0m [0m[0m2020-05-28 13:02:14,282 INFO  [org.hibernate.annotations.common.Version] (ServerService Thread Pool -- 8) HCANN000001: Hibernate Commons Annotations {5.0.5.Final}
[32muser-service     |[0m [0m[0m2020-05-28 13:02:16,241 INFO  [org.jboss.as.connector.deployers.jdbc] (MSC service thread 1-7) WFLYJCA0004: Deploying JDBC-compliant driver class org.h2.Driver (version 1.4)
[32muser-service     |[0m [0m[0m2020-05-28 13:02:16,248 INFO  [org.jboss.as.connector.deployers.jdbc] (MSC service thread 1-7) WFLYJCA0005: Deploying non-JDBC-compliant driver class org.postgresql.Driver (version 42.2)
[32muser-service     |[0m [0m[0m2020-05-28 13:02:16,289 INFO  [org.jboss.weld.Version] (MSC service thread 1-7) WELD-000900: 3.1.2 (Final)
[32muser-service     |[0m [0m[0m2020-05-28 13:02:16,423 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-6) WFLYUT0018: Host default-host starting
[32muser-service     |[0m [0m[0m2020-05-28 13:02:16,448 INFO  [org.jboss.as.connector.deployers.jdbc] (MSC service thread 1-4) WFLYJCA0018: Started Driver service with driver-name = user-service-0.4.0-SNAPSHOT.war_org.h2.Driver_1_4
[32muser-service     |[0m [0m[0m2020-05-28 13:02:16,450 INFO  [org.jboss.as.connector.deployers.jdbc] (MSC service thread 1-2) WFLYJCA0018: Started Driver service with driver-name = user-service-0.4.0-SNAPSHOT.war
[32muser-service     |[0m [0m[0m2020-05-28 13:02:16,449 INFO  [org.jboss.as.connector.deployers.jdbc] (MSC service thread 1-3) WFLYJCA0018: Started Driver service with driver-name = user-service-0.4.0-SNAPSHOT.war_org.postgresql.Driver_42_2
[32muser-service     |[0m [0m[0m2020-05-28 13:02:16,785 INFO  [org.jboss.as.jpa] (ServerService Thread Pool -- 8) WFLYJPA0010: Starting Persistence Unit (phase 2 of 2) Service 'user-service-0.4.0-SNAPSHOT.war#AUserPU'
[32muser-service     |[0m [0m[0m2020-05-28 13:02:17,172 INFO  [org.hibernate.dialect.Dialect] (ServerService Thread Pool -- 8) HHH000400: Using dialect: org.hibernate.dialect.H2Dialect
[32muser-service     |[0m [0m[0m2020-05-28 13:02:17,377 INFO  [org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl] (ServerService Thread Pool -- 8) HHH000424: Disabling contextual LOB creation as createClob() method threw error : java.lang.reflect.InvocationTargetException
[32muser-service     |[0m [0m[0m2020-05-28 13:02:17,412 INFO  [org.hibernate.envers.boot.internal.EnversServiceImpl] (ServerService Thread Pool -- 8) Envers integration enabled? : true
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,897 INFO  [stdout] (ServerService Thread Pool -- 8) Hibernate: 
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,897 INFO  [stdout] (ServerService Thread Pool -- 8)     
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,897 INFO  [stdout] (ServerService Thread Pool -- 8)     drop table AUser if exists
[33muser-database    |[0m 2020-05-28 13:02:18.905 UTC [86] ERROR:  syntax error at or near "if" at character 23
[33muser-database    |[0m 2020-05-28 13:02:18.905 UTC [86] STATEMENT:  
[33muser-database    |[0m 	    drop table AUser if exists
[33muser-database    |[0m 2020-05-28 13:02:18.914 UTC [86] ERROR:  syntax error at or near "User" at character 17
[33muser-database    |[0m 2020-05-28 13:02:18.914 UTC [86] STATEMENT:  
[33muser-database    |[0m 	    drop table User if exists
[32muser-service     |[0m [0m[33m2020-05-28 13:02:18,908 WARN  [org.hibernate.tool.schema.internal.ExceptionHandlerLoggedImpl] (ServerService Thread Pool -- 8) GenerationTarget encountered exception accepting command : Error executing DDL "
[32muser-service     |[0m     drop table AUser if exists" via JDBC Statement: org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing DDL "
[32muser-service     |[0m     drop table AUser if exists" via JDBC Statement
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:67)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.applySqlString(SchemaDropperImpl.java:375)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.applySqlStrings(SchemaDropperImpl.java:359)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.dropFromMetadata(SchemaDropperImpl.java:241)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.performDrop(SchemaDropperImpl.java:154)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.doDrop(SchemaDropperImpl.java:126)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.doDrop(SchemaDropperImpl.java:112)
[32muser-service     |[0m 	at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:144)
[32muser-service     |[0m 	at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:72)
[32muser-service     |[0m 	at org.hibernate.internal.SessionFactoryImpl.<init>(SessionFactoryImpl.java:310)
[32muser-service     |[0m 	at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:467)
[32muser-service     |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:939)
[32muser-service     |[0m 	at org.jboss.as.jpa.hibernate5.TwoPhaseBootstrapImpl.build(TwoPhaseBootstrapImpl.java:44)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1$1.run(PersistenceUnitServiceImpl.java:170)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1$1.run(PersistenceUnitServiceImpl.java:128)
[32muser-service     |[0m 	at org.wildfly.security.manager.WildFlySecurityManager.doChecked(WildFlySecurityManager.java:658)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1.run(PersistenceUnitServiceImpl.java:212)
[32muser-service     |[0m 	at org.jboss.threads.ContextClassLoaderSavingRunnable.run(ContextClassLoaderSavingRunnable.java:35)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor.safeRun(EnhancedQueueExecutor.java:1982)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.doRunTask(EnhancedQueueExecutor.java:1486)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1377)
[32muser-service     |[0m 	at java.lang.Thread.run(Thread.java:745)
[32muser-service     |[0m 	at org.jboss.threads.JBossThread.run(JBossThread.java:485)
[32muser-service     |[0m Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near "if"
[32muser-service     |[0m   Position: 23
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440)
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2183)
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:307)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:293)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:270)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:266)
[32muser-service     |[0m 	at org.jboss.jca.adapters.jdbc.WrappedStatement.execute(WrappedStatement.java:198)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:54)
[32muser-service     |[0m 	... 22 more
[32muser-service     |[0m 
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,913 INFO  [stdout] (ServerService Thread Pool -- 8) Hibernate: 
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,913 INFO  [stdout] (ServerService Thread Pool -- 8)     
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,913 INFO  [stdout] (ServerService Thread Pool -- 8)     drop table User if exists
[32muser-service     |[0m [0m[33m2020-05-28 13:02:18,914 WARN  [org.hibernate.tool.schema.internal.ExceptionHandlerLoggedImpl] (ServerService Thread Pool -- 8) GenerationTarget encountered exception accepting command : Error executing DDL "
[32muser-service     |[0m     drop table User if exists" via JDBC Statement: org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing DDL "
[32muser-service     |[0m     drop table User if exists" via JDBC Statement
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:67)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.applySqlString(SchemaDropperImpl.java:375)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.applySqlStrings(SchemaDropperImpl.java:359)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.dropFromMetadata(SchemaDropperImpl.java:241)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.performDrop(SchemaDropperImpl.java:154)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.doDrop(SchemaDropperImpl.java:126)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaDropperImpl.doDrop(SchemaDropperImpl.java:112)
[32muser-service     |[0m 	at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:144)
[32muser-service     |[0m 	at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:72)
[32muser-service     |[0m 	at org.hibernate.internal.SessionFactoryImpl.<init>(SessionFactoryImpl.java:310)
[32muser-service     |[0m 	at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:467)
[32muser-service     |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:939)
[32muser-service     |[0m 	at org.jboss.as.jpa.hibernate5.TwoPhaseBootstrapImpl.build(TwoPhaseBootstrapImpl.java:44)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1$1.run(PersistenceUnitServiceImpl.java:170)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1$1.run(PersistenceUnitServiceImpl.java:128)
[32muser-service     |[0m 	at org.wildfly.security.manager.WildFlySecurityManager.doChecked(WildFlySecurityManager.java:658)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1.run(PersistenceUnitServiceImpl.java:212)
[32muser-service     |[0m 	at org.jboss.threads.ContextClassLoaderSavingRunnable.run(ContextClassLoaderSavingRunnable.java:35)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor.safeRun(EnhancedQueueExecutor.java:1982)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.doRunTask(EnhancedQueueExecutor.java:1486)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1377)
[32muser-service     |[0m 	at java.lang.Thread.run(Thread.java:745)
[32muser-service     |[0m 	at org.jboss.threads.JBossThread.run(JBossThread.java:485)
[32muser-service     |[0m Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near "User"
[32muser-service     |[0m   Position: 17
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440)
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2183)
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:307)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:293)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:270)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:266)
[32muser-service     |[0m 	at org.jboss.jca.adapters.jdbc.WrappedStatement.execute(WrappedStatement.java:198)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:54)
[32muser-service     |[0m 	... 22 more
[32muser-service     |[0m 
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,915 INFO  [stdout] (ServerService Thread Pool -- 8) Hibernate: 
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,915 INFO  [stdout] (ServerService Thread Pool -- 8)     
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,916 INFO  [stdout] (ServerService Thread Pool -- 8)     drop sequence if exists USER_SEQ
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,947 INFO  [stdout] (ServerService Thread Pool -- 8) Hibernate: create sequence USER_SEQ start with 1 increment by 50
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,976 INFO  [stdout] (ServerService Thread Pool -- 8) Hibernate: 
[33muser-database    |[0m 2020-05-28 13:02:18.979 UTC [86] ERROR:  relation "auser" already exists
[33muser-database    |[0m 2020-05-28 13:02:18.979 UTC [86] STATEMENT:  
[33muser-database    |[0m 	    create table AUser (
[33muser-database    |[0m 	       id bigint not null,
[33muser-database    |[0m 	        email varchar(255) not null,
[33muser-database    |[0m 	        name varchar(255) not null,
[33muser-database    |[0m 	        ratingDenum integer not null,
[33muser-database    |[0m 	        ratingNum integer not null,
[33muser-database    |[0m 	        registerDate timestamp not null,
[33muser-database    |[0m 	        primary key (id)
[33muser-database    |[0m 	    )
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,976 INFO  [stdout] (ServerService Thread Pool -- 8)     
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,976 INFO  [stdout] (ServerService Thread Pool -- 8)     create table AUser (
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,976 INFO  [stdout] (ServerService Thread Pool -- 8)        id bigint not null,
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,977 INFO  [stdout] (ServerService Thread Pool -- 8)         email varchar(255) not null,
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,977 INFO  [stdout] (ServerService Thread Pool -- 8)         name varchar(255) not null,
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,977 INFO  [stdout] (ServerService Thread Pool -- 8)         ratingDenum integer not null,
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,978 INFO  [stdout] (ServerService Thread Pool -- 8)         ratingNum integer not null,
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,978 INFO  [stdout] (ServerService Thread Pool -- 8)         registerDate timestamp not null,
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,978 INFO  [stdout] (ServerService Thread Pool -- 8)         primary key (id)
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,978 INFO  [stdout] (ServerService Thread Pool -- 8)     )
[32muser-service     |[0m [0m[33m2020-05-28 13:02:18,980 WARN  [org.hibernate.tool.schema.internal.ExceptionHandlerLoggedImpl] (ServerService Thread Pool -- 8) GenerationTarget encountered exception accepting command : Error executing DDL "
[32muser-service     |[0m     create table AUser (
[32muser-service     |[0m        id bigint not null,
[32muser-service     |[0m         email varchar(255) not null,
[32muser-service     |[0m         name varchar(255) not null,
[32muser-service     |[0m         ratingDenum integer not null,
[32muser-service     |[0m         ratingNum integer not null,
[32muser-service     |[0m         registerDate timestamp not null,
[32muser-service     |[0m         primary key (id)
[32muser-service     |[0m     )" via JDBC Statement: org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing DDL "
[32muser-service     |[0m     create table AUser (
[32muser-service     |[0m        id bigint not null,
[32muser-service     |[0m         email varchar(255) not null,
[32muser-service     |[0m         name varchar(255) not null,
[32muser-service     |[0m         ratingDenum integer not null,
[32muser-service     |[0m         ratingNum integer not null,
[32muser-service     |[0m         registerDate timestamp not null,
[32muser-service     |[0m         primary key (id)
[32muser-service     |[0m     )" via JDBC Statement
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:67)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlString(SchemaCreatorImpl.java:440)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlStrings(SchemaCreatorImpl.java:424)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.createFromMetadata(SchemaCreatorImpl.java:315)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.performCreation(SchemaCreatorImpl.java:166)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:135)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:121)
[32muser-service     |[0m 	at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:155)
[32muser-service     |[0m 	at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:72)
[32muser-service     |[0m 	at org.hibernate.internal.SessionFactoryImpl.<init>(SessionFactoryImpl.java:310)
[32muser-service     |[0m 	at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:467)
[32muser-service     |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:939)
[32muser-service     |[0m 	at org.jboss.as.jpa.hibernate5.TwoPhaseBootstrapImpl.build(TwoPhaseBootstrapImpl.java:44)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1$1.run(PersistenceUnitServiceImpl.java:170)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1$1.run(PersistenceUnitServiceImpl.java:128)
[32muser-service     |[0m 	at org.wildfly.security.manager.WildFlySecurityManager.doChecked(WildFlySecurityManager.java:658)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1.run(PersistenceUnitServiceImpl.java:212)
[32muser-service     |[0m 	at org.jboss.threads.ContextClassLoaderSavingRunnable.run(ContextClassLoaderSavingRunnable.java:35)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor.safeRun(EnhancedQueueExecutor.java:1982)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.doRunTask(EnhancedQueueExecutor.java:1486)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1377)
[32muser-service     |[0m 	at java.lang.Thread.run(Thread.java:745)
[32muser-service     |[0m 	at org.jboss.threads.JBossThread.run(JBossThread.java:485)
[32muser-service     |[0m Caused by: org.postgresql.util.PSQLException: ERROR: relation "auser" already exists
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440)
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2183)
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:307)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:293)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:270)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:266)
[32muser-service     |[0m 	at org.jboss.jca.adapters.jdbc.WrappedStatement.execute(WrappedStatement.java:198)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:54)
[32muser-service     |[0m 	... 22 more
[32muser-service     |[0m 
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,995 INFO  [stdout] (ServerService Thread Pool -- 8) Hibernate: 
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,995 INFO  [stdout] (ServerService Thread Pool -- 8)     
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,995 INFO  [stdout] (ServerService Thread Pool -- 8)     create table User (
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,996 INFO  [stdout] (ServerService Thread Pool -- 8)        id bigint not null,
[33muser-database    |[0m 2020-05-28 13:02:18.999 UTC [86] ERROR:  syntax error at or near "User" at character 19
[33muser-database    |[0m 2020-05-28 13:02:18.999 UTC [86] STATEMENT:  
[33muser-database    |[0m 	    create table User (
[33muser-database    |[0m 	       id bigint not null,
[33muser-database    |[0m 	        email varchar(255) not null,
[33muser-database    |[0m 	        name varchar(255) not null,
[33muser-database    |[0m 	        ratingDenum integer not null,
[33muser-database    |[0m 	        ratingNum integer not null,
[33muser-database    |[0m 	        registerDate timestamp not null,
[33muser-database    |[0m 	        primary key (id)
[33muser-database    |[0m 	    )
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,996 INFO  [stdout] (ServerService Thread Pool -- 8)         email varchar(255) not null,
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,997 INFO  [stdout] (ServerService Thread Pool -- 8)         name varchar(255) not null,
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,997 INFO  [stdout] (ServerService Thread Pool -- 8)         ratingDenum integer not null,
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,998 INFO  [stdout] (ServerService Thread Pool -- 8)         ratingNum integer not null,
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,998 INFO  [stdout] (ServerService Thread Pool -- 8)         registerDate timestamp not null,
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,998 INFO  [stdout] (ServerService Thread Pool -- 8)         primary key (id)
[32muser-service     |[0m [0m[0m2020-05-28 13:02:18,998 INFO  [stdout] (ServerService Thread Pool -- 8)     )
[32muser-service     |[0m [0m[33m2020-05-28 13:02:19,000 WARN  [org.hibernate.tool.schema.internal.ExceptionHandlerLoggedImpl] (ServerService Thread Pool -- 8) GenerationTarget encountered exception accepting command : Error executing DDL "
[32muser-service     |[0m     create table User (
[32muser-service     |[0m        id bigint not null,
[32muser-service     |[0m         email varchar(255) not null,
[32muser-service     |[0m         name varchar(255) not null,
[32muser-service     |[0m         ratingDenum integer not null,
[32muser-service     |[0m         ratingNum integer not null,
[32muser-service     |[0m         registerDate timestamp not null,
[32muser-service     |[0m         primary key (id)
[32muser-service     |[0m     )" via JDBC Statement: org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing DDL "
[32muser-service     |[0m     create table User (
[32muser-service     |[0m        id bigint not null,
[32muser-service     |[0m         email varchar(255) not null,
[32muser-service     |[0m         name varchar(255) not null,
[32muser-service     |[0m         ratingDenum integer not null,
[32muser-service     |[0m         ratingNum integer not null,
[32muser-service     |[0m         registerDate timestamp not null,
[32muser-service     |[0m         primary key (id)
[32muser-service     |[0m     )" via JDBC Statement
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:67)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlString(SchemaCreatorImpl.java:440)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlStrings(SchemaCreatorImpl.java:424)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.createFromMetadata(SchemaCreatorImpl.java:315)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.performCreation(SchemaCreatorImpl.java:166)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:135)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:121)
[32muser-service     |[0m 	at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:155)
[32muser-service     |[0m 	at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:72)
[32muser-service     |[0m 	at org.hibernate.internal.SessionFactoryImpl.<init>(SessionFactoryImpl.java:310)
[32muser-service     |[0m 	at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:467)
[32muser-service     |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:939)
[32muser-service     |[0m 	at org.jboss.as.jpa.hibernate5.TwoPhaseBootstrapImpl.build(TwoPhaseBootstrapImpl.java:44)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1$1.run(PersistenceUnitServiceImpl.java:170)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1$1.run(PersistenceUnitServiceImpl.java:128)
[32muser-service     |[0m 	at org.wildfly.security.manager.WildFlySecurityManager.doChecked(WildFlySecurityManager.java:658)
[32muser-service     |[0m 	at org.jboss.as.jpa.service.PersistenceUnitServiceImpl$1.run(PersistenceUnitServiceImpl.java:212)
[32muser-service     |[0m 	at org.jboss.threads.ContextClassLoaderSavingRunnable.run(ContextClassLoaderSavingRunnable.java:35)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor.safeRun(EnhancedQueueExecutor.java:1982)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.doRunTask(EnhancedQueueExecutor.java:1486)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1377)
[32muser-service     |[0m 	at java.lang.Thread.run(Thread.java:745)
[32muser-service     |[0m 	at org.jboss.threads.JBossThread.run(JBossThread.java:485)
[32muser-service     |[0m Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near "User"
[32muser-service     |[0m   Position: 19
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440)
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2183)
[32muser-service     |[0m 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:307)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:293)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:270)
[32muser-service     |[0m 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:266)
[32muser-service     |[0m 	at org.jboss.jca.adapters.jdbc.WrappedStatement.execute(WrappedStatement.java:198)
[32muser-service     |[0m 	at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:54)
[32muser-service     |[0m 	... 22 more
[32muser-service     |[0m 
[32muser-service     |[0m [0m[0m2020-05-28 13:02:19,022 INFO  [org.hibernate.tool.schema.internal.SchemaCreatorImpl] (ServerService Thread Pool -- 8) HHH000476: Executing import script 'org.hibernate.tool.schema.internal.exec.ScriptSourceInputNonExistentImpl@5fa94080'
[32muser-service     |[0m [0m[0m2020-05-28 13:02:19,558 INFO  [org.aerogear.kafka.cdi.extension.KafkaExtension] (MSC service thread 1-5) setting bootstrap.servers IP for, #{thorntail.kafka-configuration.host}:#{thorntail.kafka-configuration.port}
[32muser-service     |[0m [0m[0m2020-05-28 13:02:20,109 INFO  [org.apache.kafka.clients.consumer.ConsumerConfig] (MSC service thread 1-5) ConsumerConfig values: 
[32muser-service     |[0m 	auto.commit.interval.ms = 5000
[32muser-service     |[0m 	auto.offset.reset = latest
[32muser-service     |[0m 	bootstrap.servers = [kafka:9092]
[32muser-service     |[0m 	check.crcs = true
[32muser-service     |[0m 	client.id = 
[32muser-service     |[0m 	connections.max.idle.ms = 540000
[32muser-service     |[0m 	enable.auto.commit = true
[32muser-service     |[0m 	exclude.internal.topics = true
[32muser-service     |[0m 	fetch.max.bytes = 52428800
[32muser-service     |[0m 	fetch.max.wait.ms = 500
[32muser-service     |[0m 	fetch.min.bytes = 1
[32muser-service     |[0m 	group.id = pinfo-microservices
[32muser-service     |[0m 	heartbeat.interval.ms = 3000
[32muser-service     |[0m 	interceptor.classes = null
[32muser-service     |[0m 	internal.leave.group.on.close = true
[32muser-service     |[0m 	isolation.level = read_uncommitted
[32muser-service     |[0m 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
[32muser-service     |[0m 	max.partition.fetch.bytes = 1048576
[32muser-service     |[0m 	max.poll.interval.ms = 300000
[32muser-service     |[0m 	max.poll.records = 500
[32muser-service     |[0m 	metadata.max.age.ms = 300000
[32muser-service     |[0m 	metric.reporters = []
[32muser-service     |[0m 	metrics.num.samples = 2
[32muser-service     |[0m 	metrics.recording.level = INFO
[32muser-service     |[0m 	metrics.sample.window.ms = 30000
[32muser-service     |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[32muser-service     |[0m 	receive.buffer.bytes = 65536
[32muser-service     |[0m 	reconnect.backoff.max.ms = 1000
[32muser-service     |[0m 	reconnect.backoff.ms = 50
[32muser-service     |[0m 	request.timeout.ms = 305000
[32muser-service     |[0m 	retry.backoff.ms = 100
[32muser-service     |[0m 	sasl.jaas.config = null
[32muser-service     |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[32muser-service     |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[32muser-service     |[0m 	sasl.kerberos.service.name = null
[32muser-service     |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[32muser-service     |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[32muser-service     |[0m 	sasl.mechanism = GSSAPI
[32muser-service     |[0m 	security.protocol = PLAINTEXT
[32muser-service     |[0m 	send.buffer.bytes = 131072
[32muser-service     |[0m 	session.timeout.ms = 10000
[32muser-service     |[0m 	ssl.cipher.suites = null
[32muser-service     |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[32muser-service     |[0m 	ssl.endpoint.identification.algorithm = null
[32muser-service     |[0m 	ssl.key.password = null
[32muser-service     |[0m 	ssl.keymanager.algorithm = SunX509
[32muser-service     |[0m 	ssl.keystore.location = null
[32muser-service     |[0m 	ssl.keystore.password = null
[32muser-service     |[0m 	ssl.keystore.type = JKS
[32muser-service     |[0m 	ssl.protocol = TLS
[32muser-service     |[0m 	ssl.provider = null
[32muser-service     |[0m 	ssl.secure.random.implementation = null
[32muser-service     |[0m 	ssl.trustmanager.algorithm = PKIX
[32muser-service     |[0m 	ssl.truststore.location = null
[32muser-service     |[0m 	ssl.truststore.password = null
[32muser-service     |[0m 	ssl.truststore.type = JKS
[32muser-service     |[0m 	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
[32muser-service     |[0m 
[32muser-service     |[0m [0m[0m2020-05-28 13:02:20,190 INFO  [org.apache.kafka.common.utils.AppInfoParser] (MSC service thread 1-5) Kafka version : 1.0.0
[32muser-service     |[0m [0m[0m2020-05-28 13:02:20,190 INFO  [org.apache.kafka.common.utils.AppInfoParser] (MSC service thread 1-5) Kafka commitId : aaa7af6d4a11b29d
[32muser-service     |[0m [0m[0m2020-05-28 13:02:20,584 INFO  [org.apache.kafka.clients.consumer.internals.AbstractCoordinator] (EE-ManagedExecutorService-default-Thread-1) [Consumer clientId=consumer-1, groupId=pinfo-microservices] Discovered coordinator kafka:9092 (id: 2147483646 rack: null)
[32muser-service     |[0m [0m[0m2020-05-28 13:02:20,603 INFO  [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] (EE-ManagedExecutorService-default-Thread-1) [Consumer clientId=consumer-1, groupId=pinfo-microservices] Revoking previously assigned partitions []
[32muser-service     |[0m [0m[0m2020-05-28 13:02:20,613 INFO  [org.apache.kafka.clients.consumer.internals.AbstractCoordinator] (EE-ManagedExecutorService-default-Thread-1) [Consumer clientId=consumer-1, groupId=pinfo-microservices] (Re-)joining group
[36mkafka            |[0m [2020-05-28 13:02:20,668] INFO [GroupCoordinator 1]: Preparing to rebalance group pinfo-microservices in state PreparingRebalance with old generation 62 (__consumer_offsets-9) (reason: Adding new member consumer-1-4fe87fe4-2431-4818-8e9f-3b93235058eb) (kafka.coordinator.group.GroupCoordinator)
[32muser-service     |[0m [0m[0m2020-05-28 13:02:21,040 INFO  [org.jboss.resteasy.resteasy_jaxrs.i18n] (ServerService Thread Pool -- 8) RESTEASY002225: Deploying javax.ws.rs.core.Application: class api.rest.JaxRsActivator
[32muser-service     |[0m [0m[33m2020-05-28 13:02:21,044 WARN  [org.jboss.as.weld] (ServerService Thread Pool -- 8) WFLYWELD0052: Using deployment classloader to load proxy classes for module io.swagger. Package-private access will not work. To fix this the module should declare dependencies on [org.jboss.weld.core, org.jboss.weld.spi, org.jboss.weld.api]
[32muser-service     |[0m [0m[0m2020-05-28 13:02:21,148 INFO  [org.wildfly.extension.undertow] (ServerService Thread Pool -- 8) WFLYUT0021: Registered web context: '/' for server 'default-server'
[32muser-service     |[0m [0m[0m2020-05-28 13:02:21,274 INFO  [org.jboss.as.server] (main) WFLYSRV0010: Deployed "user-service-0.4.0-SNAPSHOT.war" (runtime-name : "user-service-0.4.0-SNAPSHOT.war")
[32muser-service     |[0m [0m[0m2020-05-28 13:02:21,305 INFO  [org.wildfly.swarm] (main) THORN99999: Thorntail is Ready
[36mkafka            |[0m [2020-05-28 13:02:23,682] INFO [GroupCoordinator 1]: Stabilized group pinfo-microservices generation 63 (__consumer_offsets-9) (kafka.coordinator.group.GroupCoordinator)
[36mkafka            |[0m [2020-05-28 13:02:23,696] INFO [GroupCoordinator 1]: Assignment received from leader for group pinfo-microservices for generation 63 (kafka.coordinator.group.GroupCoordinator)
[32muser-service     |[0m [0m[0m2020-05-28 13:02:23,705 INFO  [org.apache.kafka.clients.consumer.internals.AbstractCoordinator] (EE-ManagedExecutorService-default-Thread-1) [Consumer clientId=consumer-1, groupId=pinfo-microservices] Successfully joined group with generation 63
[32muser-service     |[0m [0m[0m2020-05-28 13:02:23,708 INFO  [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] (EE-ManagedExecutorService-default-Thread-1) [Consumer clientId=consumer-1, groupId=pinfo-microservices] Setting newly assigned partitions [usersReq-0]
[32muser-service     |[0m [0m[0m2020-05-28 13:03:46,264 INFO  [org.hibernate.hql.internal.QueryTranslatorFactoryInitiator] (default task-1) HHH000397: Using ASTQueryTranslatorFactory
[32muser-service     |[0m [0m[0m2020-05-28 13:03:46,516 INFO  [stdout] (default task-1) Hibernate: 
[32muser-service     |[0m [0m[0m2020-05-28 13:03:46,516 INFO  [stdout] (default task-1)     select
[32muser-service     |[0m [0m[0m2020-05-28 13:03:46,516 INFO  [stdout] (default task-1)         auser0_.id as id1_0_,
[32muser-service     |[0m [0m[0m2020-05-28 13:03:46,517 INFO  [stdout] (default task-1)         auser0_.email as email2_0_,
[32muser-service     |[0m [0m[0m2020-05-28 13:03:46,517 INFO  [stdout] (default task-1)         auser0_.name as name3_0_,
[32muser-service     |[0m [0m[0m2020-05-28 13:03:46,517 INFO  [stdout] (default task-1)         auser0_.ratingDenum as ratingDe4_0_,
[32muser-service     |[0m [0m[0m2020-05-28 13:03:46,517 INFO  [stdout] (default task-1)         auser0_.ratingNum as ratingNu5_0_,
[32muser-service     |[0m [0m[0m2020-05-28 13:03:46,517 INFO  [stdout] (default task-1)         auser0_.registerDate as register6_0_ 
[32muser-service     |[0m [0m[0m2020-05-28 13:03:46,517 INFO  [stdout] (default task-1)     from
[32muser-service     |[0m [0m[0m2020-05-28 13:03:46,518 INFO  [stdout] (default task-1)         AUser auser0_
[32muser-service     |[0m [0m[31m2020-05-28 13:03:46,616 ERROR [io.undertow.request] (default task-1) UT005023: Exception handling request to /user: org.jboss.resteasy.spi.UnhandledException: javax.persistence.PersistenceException: org.hibernate.InstantiationException: Cannot instantiate abstract class or interface:  : domain.model.AUser
[32muser-service     |[0m 	at org.jboss.resteasy.core.ExceptionHandler.handleApplicationException(ExceptionHandler.java:82)
[32muser-service     |[0m 	at org.jboss.resteasy.core.ExceptionHandler.handleException(ExceptionHandler.java:346)
[32muser-service     |[0m 	at org.jboss.resteasy.core.SynchronousDispatcher.writeException(SynchronousDispatcher.java:193)
[32muser-service     |[0m 	at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:456)
[32muser-service     |[0m 	at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:229)
[32muser-service     |[0m 	at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:135)
[32muser-service     |[0m 	at org.jboss.resteasy.core.interception.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:356)
[32muser-service     |[0m 	at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:138)
[32muser-service     |[0m 	at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:215)
[32muser-service     |[0m 	at org.jboss.resteasy.plugins.server.servlet.ServletContainerDispatcher.service(ServletContainerDispatcher.java:227)
[32muser-service     |[0m 	at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service(HttpServletDispatcher.java:56)
[32muser-service     |[0m 	at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service(HttpServletDispatcher.java:51)
[32muser-service     |[0m 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:590)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:74)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.ServletChain$1.handleRequest(ServletChain.java:68)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36)
[32muser-service     |[0m 	at org.wildfly.extension.undertow.security.SecurityContextAssociationHandler.handleRequest(SecurityContextAssociationHandler.java:78)
[32muser-service     |[0m 	at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
[32muser-service     |[0m 	at org.wildfly.swarm.generated.FaviconErrorHandler.handleRequest(FaviconErrorHandler.java:61)
[32muser-service     |[0m 	at io.undertow.server.handlers.PathHandler.handleRequest(PathHandler.java:91)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.RedirectDirHandler.handleRequest(RedirectDirHandler.java:68)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:132)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57)
[32muser-service     |[0m 	at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
[32muser-service     |[0m 	at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64)
[32muser-service     |[0m 	at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77)
[32muser-service     |[0m 	at io.undertow.security.handlers.NotificationReceiverHandler.handleRequest(NotificationReceiverHandler.java:50)
[32muser-service     |[0m 	at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43)
[32muser-service     |[0m 	at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
[32muser-service     |[0m 	at org.wildfly.extension.undertow.security.jacc.JACCContextIdHandler.handleRequest(JACCContextIdHandler.java:61)
[32muser-service     |[0m 	at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
[32muser-service     |[0m 	at org.wildfly.extension.undertow.deployment.GlobalRequestControllerHandler.handleRequest(GlobalRequestControllerHandler.java:68)
[32muser-service     |[0m 	at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:269)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:78)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:133)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:130)
[32muser-service     |[0m 	at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48)
[32muser-service     |[0m 	at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43)
[32muser-service     |[0m 	at org.wildfly.extension.undertow.security.SecurityContextThreadSetupAction.lambda$create$0(SecurityContextThreadSetupAction.java:105)
[32muser-service     |[0m 	at org.wildfly.extension.undertow.deployment.UndertowDeploymentInfoService$UndertowThreadSetupAction.lambda$create$0(UndertowDeploymentInfoService.java:1504)
[32muser-service     |[0m 	at org.wildfly.extension.undertow.deployment.UndertowDeploymentInfoService$UndertowThreadSetupAction.lambda$create$0(UndertowDeploymentInfoService.java:1504)
[32muser-service     |[0m 	at org.wildfly.extension.undertow.deployment.UndertowDeploymentInfoService$UndertowThreadSetupAction.lambda$create$0(UndertowDeploymentInfoService.java:1504)
[32muser-service     |[0m 	at org.wildfly.extension.undertow.deployment.UndertowDeploymentInfoService$UndertowThreadSetupAction.lambda$create$0(UndertowDeploymentInfoService.java:1504)
[32muser-service     |[0m 	at org.wildfly.extension.undertow.deployment.UndertowDeploymentInfoService$UndertowThreadSetupAction.lambda$create$0(UndertowDeploymentInfoService.java:1504)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:249)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:78)
[32muser-service     |[0m 	at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:99)
[32muser-service     |[0m 	at io.undertow.server.Connectors.executeRootHandler(Connectors.java:376)
[32muser-service     |[0m 	at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:830)
[32muser-service     |[0m 	at org.jboss.threads.ContextClassLoaderSavingRunnable.run(ContextClassLoaderSavingRunnable.java:35)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor.safeRun(EnhancedQueueExecutor.java:1982)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.doRunTask(EnhancedQueueExecutor.java:1486)
[32muser-service     |[0m 	at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1348)
[32muser-service     |[0m 	at java.lang.Thread.run(Thread.java:745)
[32muser-service     |[0m Caused by: javax.persistence.PersistenceException: org.hibernate.InstantiationException: Cannot instantiate abstract class or interface:  : domain.model.AUser
[32muser-service     |[0m 	at org.hibernate.internal.ExceptionConverterImpl.convert(ExceptionConverterImpl.java:154)
[32muser-service     |[0m 	at org.hibernate.query.internal.AbstractProducedQuery.list(AbstractProducedQuery.java:1515)
[32muser-service     |[0m 	at org.hibernate.query.Query.getResultList(Query.java:132)
[32muser-service     |[0m 	at org.hibernate.query.criteria.internal.compile.CriteriaQueryTypeQueryAdapter.getResultList(CriteriaQueryTypeQueryAdapter.java:74)
[32muser-service     |[0m 	at org.jboss.as.jpa.container.TypedQueryNonTxInvocationDetacher.getResultList(TypedQueryNonTxInvocationDetacher.java:58)
[32muser-service     |[0m 	at domain.service.UserServiceImpl.getAll(UserServiceImpl.java:34)
[32muser-service     |[0m 	at domain.service.UserServiceImpl$Proxy$_$$_WeldClientProxy.getAll(Unknown Source)
[32muser-service     |[0m 	at api.rest.UserRestService.getAll(UserRestService.java:39)
[32muser-service     |[0m 	at api.rest.UserRestService$Proxy$_$$_WeldClientProxy.getAll(Unknown Source)
[32muser-service     |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[32muser-service     |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[32muser-service     |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[32muser-service     |[0m 	at java.lang.reflect.Method.invoke(Method.java:498)
[32muser-service     |[0m 	at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:138)
[32muser-service     |[0m 	at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:517)
[32muser-service     |[0m 	at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:406)
[32muser-service     |[0m 	at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$0(ResourceMethodInvoker.java:370)
[32muser-service     |[0m 	at org.jboss.resteasy.core.interception.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:356)
[32muser-service     |[0m 	at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:372)
[32muser-service     |[0m 	at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:344)
[32muser-service     |[0m 	at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:317)
[32muser-service     |[0m 	at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:440)
[32muser-service     |[0m 	... 54 more
[32muser-service     |[0m Caused by: org.hibernate.InstantiationException: Cannot instantiate abstract class or interface:  : domain.model.AUser
[32muser-service     |[0m 	at org.hibernate.tuple.PojoInstantiator.instantiate(PojoInstantiator.java:79)
[32muser-service     |[0m 	at org.hibernate.tuple.PojoInstantiator.instantiate(PojoInstantiator.java:105)
[32muser-service     |[0m 	at org.hibernate.tuple.entity.AbstractEntityTuplizer.instantiate(AbstractEntityTuplizer.java:685)
[32muser-service     |[0m 	at org.hibernate.persister.entity.AbstractEntityPersister.instantiate(AbstractEntityPersister.java:5027)
[32muser-service     |[0m 	at org.hibernate.internal.SessionImpl.instantiate(SessionImpl.java:1711)
[32muser-service     |[0m 	at org.hibernate.internal.SessionImpl.instantiate(SessionImpl.java:1695)
[32muser-service     |[0m 	at org.hibernate.loader.Loader.instanceNotYetLoaded(Loader.java:1736)
[32muser-service     |[0m 	at org.hibernate.loader.Loader.getRow(Loader.java:1598)
[32muser-service     |[0m 	at org.hibernate.loader.Loader.getRowFromResultSet(Loader.java:742)
[32muser-service     |[0m 	at org.hibernate.loader.Loader.processResultSet(Loader.java:1002)
[32muser-service     |[0m 	at org.hibernate.loader.Loader.doQuery(Loader.java:960)
[32muser-service     |[0m 	at org.hibernate.loader.Loader.doQueryAndInitializeNonLazyCollections(Loader.java:351)
[32muser-service     |[0m 	at org.hibernate.loader.Loader.doList(Loader.java:2787)
[32muser-service     |[0m 	at org.hibernate.loader.Loader.doList(Loader.java:2770)
[32muser-service     |[0m 	at org.hibernate.loader.Loader.listIgnoreQueryCache(Loader.java:2604)
[32muser-service     |[0m 	at org.hibernate.loader.Loader.list(Loader.java:2599)
[32muser-service     |[0m 	at org.hibernate.loader.hql.QueryLoader.list(QueryLoader.java:505)
[32muser-service     |[0m 	at org.hibernate.hql.internal.ast.QueryTranslatorImpl.list(QueryTranslatorImpl.java:395)
[32muser-service     |[0m 	at org.hibernate.engine.query.spi.HQLQueryPlan.performList(HQLQueryPlan.java:220)
[32muser-service     |[0m 	at org.hibernate.internal.SessionImpl.list(SessionImpl.java:1537)
[32muser-service     |[0m 	at org.hibernate.query.internal.AbstractProducedQuery.doList(AbstractProducedQuery.java:1538)
[32muser-service     |[0m 	at org.hibernate.query.internal.AbstractProducedQuery.list(AbstractProducedQuery.java:1506)
[32muser-service     |[0m 	... 74 more
[32muser-service     |[0m 
Stopping user-service  ... 
Stopping kafka         ... 
Stopping zookeeper     ... 
Stopping user-database ... 
[4A[2KStopping user-service  ... [32mdone[0m[4B[3A[2KStopping kafka         ... [32mdone[0m[3B[2A[2KStopping zookeeper     ... [32mdone[0m[2B[1A[2KStopping user-database ... [32mdone[0m[1BGracefully stopping... (press Ctrl+C again to force)
